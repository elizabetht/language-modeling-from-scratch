{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7417baef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from accelerate) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from accelerate) (2.9.0+cu130)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.12.0)\n",
      "Requirement already satisfied: requests in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.6)\n",
      "Requirement already satisfied: jinja2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.13.0.50)\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (13.0.0.19)\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.0.0.15)\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.4.0.35)\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.0.3.29)\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.2.49)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.8.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.27.7)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.24)\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (13.0.39)\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (13.0.39)\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.15.0.42)\n",
      "Requirement already satisfied: triton==3.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10e5b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 81.59it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Load model with optimizations to prevent kernel crash:\n",
    "# 1. Use torch_dtype=torch.bfloat16 to reduce memory usage by 50%\n",
    "# 2. Use device_map=\"auto\" to automatically distribute model across available devices\n",
    "# 3. Use low_cpu_mem_usage=True to minimize CPU memory during loading\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63b194c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c274bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   9906,     11,    856,    836,    374,  23880,    323,    358,\n",
       "           1097,    264,    220,   1187,   1060,   2362,   8954]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# Move inputs to the same device as model immediately\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6896908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Maria and I am a 24 year old female\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbfc385b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'': 16060522752,\n",
       "             'model': 15009849600,\n",
       "             'model.embed_tokens': 1050673152,\n",
       "             'model.embed_tokens.weight': 1050673152,\n",
       "             'model.layers': 13959168000,\n",
       "             'model.layers.0': 436224000,\n",
       "             'model.layers.0.self_attn': 83886080,\n",
       "             'model.layers.0.self_attn.q_proj': 33554432,\n",
       "             'model.layers.0.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.0.self_attn.k_proj': 8388608,\n",
       "             'model.layers.0.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.0.self_attn.v_proj': 8388608,\n",
       "             'model.layers.0.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.0.self_attn.o_proj': 33554432,\n",
       "             'model.layers.0.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.0.mlp': 352321536,\n",
       "             'model.layers.0.mlp.gate_proj': 117440512,\n",
       "             'model.layers.0.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.0.mlp.up_proj': 117440512,\n",
       "             'model.layers.0.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.0.mlp.down_proj': 117440512,\n",
       "             'model.layers.0.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.0.input_layernorm': 8192,\n",
       "             'model.layers.0.input_layernorm.weight': 8192,\n",
       "             'model.layers.0.post_attention_layernorm': 8192,\n",
       "             'model.layers.0.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.1': 436224000,\n",
       "             'model.layers.1.self_attn': 83886080,\n",
       "             'model.layers.1.self_attn.q_proj': 33554432,\n",
       "             'model.layers.1.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.1.self_attn.k_proj': 8388608,\n",
       "             'model.layers.1.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.1.self_attn.v_proj': 8388608,\n",
       "             'model.layers.1.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.1.self_attn.o_proj': 33554432,\n",
       "             'model.layers.1.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.1.mlp': 352321536,\n",
       "             'model.layers.1.mlp.gate_proj': 117440512,\n",
       "             'model.layers.1.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.1.mlp.up_proj': 117440512,\n",
       "             'model.layers.1.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.1.mlp.down_proj': 117440512,\n",
       "             'model.layers.1.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.1.input_layernorm': 8192,\n",
       "             'model.layers.1.input_layernorm.weight': 8192,\n",
       "             'model.layers.1.post_attention_layernorm': 8192,\n",
       "             'model.layers.1.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.2': 436224000,\n",
       "             'model.layers.2.self_attn': 83886080,\n",
       "             'model.layers.2.self_attn.q_proj': 33554432,\n",
       "             'model.layers.2.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.2.self_attn.k_proj': 8388608,\n",
       "             'model.layers.2.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.2.self_attn.v_proj': 8388608,\n",
       "             'model.layers.2.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.2.self_attn.o_proj': 33554432,\n",
       "             'model.layers.2.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.2.mlp': 352321536,\n",
       "             'model.layers.2.mlp.gate_proj': 117440512,\n",
       "             'model.layers.2.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.2.mlp.up_proj': 117440512,\n",
       "             'model.layers.2.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.2.mlp.down_proj': 117440512,\n",
       "             'model.layers.2.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.2.input_layernorm': 8192,\n",
       "             'model.layers.2.input_layernorm.weight': 8192,\n",
       "             'model.layers.2.post_attention_layernorm': 8192,\n",
       "             'model.layers.2.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.3': 436224000,\n",
       "             'model.layers.3.self_attn': 83886080,\n",
       "             'model.layers.3.self_attn.q_proj': 33554432,\n",
       "             'model.layers.3.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.3.self_attn.k_proj': 8388608,\n",
       "             'model.layers.3.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.3.self_attn.v_proj': 8388608,\n",
       "             'model.layers.3.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.3.self_attn.o_proj': 33554432,\n",
       "             'model.layers.3.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.3.mlp': 352321536,\n",
       "             'model.layers.3.mlp.gate_proj': 117440512,\n",
       "             'model.layers.3.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.3.mlp.up_proj': 117440512,\n",
       "             'model.layers.3.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.3.mlp.down_proj': 117440512,\n",
       "             'model.layers.3.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.3.input_layernorm': 8192,\n",
       "             'model.layers.3.input_layernorm.weight': 8192,\n",
       "             'model.layers.3.post_attention_layernorm': 8192,\n",
       "             'model.layers.3.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.4': 436224000,\n",
       "             'model.layers.4.self_attn': 83886080,\n",
       "             'model.layers.4.self_attn.q_proj': 33554432,\n",
       "             'model.layers.4.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.4.self_attn.k_proj': 8388608,\n",
       "             'model.layers.4.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.4.self_attn.v_proj': 8388608,\n",
       "             'model.layers.4.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.4.self_attn.o_proj': 33554432,\n",
       "             'model.layers.4.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.4.mlp': 352321536,\n",
       "             'model.layers.4.mlp.gate_proj': 117440512,\n",
       "             'model.layers.4.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.4.mlp.up_proj': 117440512,\n",
       "             'model.layers.4.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.4.mlp.down_proj': 117440512,\n",
       "             'model.layers.4.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.4.input_layernorm': 8192,\n",
       "             'model.layers.4.input_layernorm.weight': 8192,\n",
       "             'model.layers.4.post_attention_layernorm': 8192,\n",
       "             'model.layers.4.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.5': 436224000,\n",
       "             'model.layers.5.self_attn': 83886080,\n",
       "             'model.layers.5.self_attn.q_proj': 33554432,\n",
       "             'model.layers.5.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.5.self_attn.k_proj': 8388608,\n",
       "             'model.layers.5.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.5.self_attn.v_proj': 8388608,\n",
       "             'model.layers.5.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.5.self_attn.o_proj': 33554432,\n",
       "             'model.layers.5.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.5.mlp': 352321536,\n",
       "             'model.layers.5.mlp.gate_proj': 117440512,\n",
       "             'model.layers.5.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.5.mlp.up_proj': 117440512,\n",
       "             'model.layers.5.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.5.mlp.down_proj': 117440512,\n",
       "             'model.layers.5.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.5.input_layernorm': 8192,\n",
       "             'model.layers.5.input_layernorm.weight': 8192,\n",
       "             'model.layers.5.post_attention_layernorm': 8192,\n",
       "             'model.layers.5.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.6': 436224000,\n",
       "             'model.layers.6.self_attn': 83886080,\n",
       "             'model.layers.6.self_attn.q_proj': 33554432,\n",
       "             'model.layers.6.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.6.self_attn.k_proj': 8388608,\n",
       "             'model.layers.6.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.6.self_attn.v_proj': 8388608,\n",
       "             'model.layers.6.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.6.self_attn.o_proj': 33554432,\n",
       "             'model.layers.6.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.6.mlp': 352321536,\n",
       "             'model.layers.6.mlp.gate_proj': 117440512,\n",
       "             'model.layers.6.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.6.mlp.up_proj': 117440512,\n",
       "             'model.layers.6.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.6.mlp.down_proj': 117440512,\n",
       "             'model.layers.6.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.6.input_layernorm': 8192,\n",
       "             'model.layers.6.input_layernorm.weight': 8192,\n",
       "             'model.layers.6.post_attention_layernorm': 8192,\n",
       "             'model.layers.6.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.7': 436224000,\n",
       "             'model.layers.7.self_attn': 83886080,\n",
       "             'model.layers.7.self_attn.q_proj': 33554432,\n",
       "             'model.layers.7.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.7.self_attn.k_proj': 8388608,\n",
       "             'model.layers.7.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.7.self_attn.v_proj': 8388608,\n",
       "             'model.layers.7.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.7.self_attn.o_proj': 33554432,\n",
       "             'model.layers.7.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.7.mlp': 352321536,\n",
       "             'model.layers.7.mlp.gate_proj': 117440512,\n",
       "             'model.layers.7.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.7.mlp.up_proj': 117440512,\n",
       "             'model.layers.7.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.7.mlp.down_proj': 117440512,\n",
       "             'model.layers.7.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.7.input_layernorm': 8192,\n",
       "             'model.layers.7.input_layernorm.weight': 8192,\n",
       "             'model.layers.7.post_attention_layernorm': 8192,\n",
       "             'model.layers.7.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.8': 436224000,\n",
       "             'model.layers.8.self_attn': 83886080,\n",
       "             'model.layers.8.self_attn.q_proj': 33554432,\n",
       "             'model.layers.8.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.8.self_attn.k_proj': 8388608,\n",
       "             'model.layers.8.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.8.self_attn.v_proj': 8388608,\n",
       "             'model.layers.8.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.8.self_attn.o_proj': 33554432,\n",
       "             'model.layers.8.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.8.mlp': 352321536,\n",
       "             'model.layers.8.mlp.gate_proj': 117440512,\n",
       "             'model.layers.8.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.8.mlp.up_proj': 117440512,\n",
       "             'model.layers.8.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.8.mlp.down_proj': 117440512,\n",
       "             'model.layers.8.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.8.input_layernorm': 8192,\n",
       "             'model.layers.8.input_layernorm.weight': 8192,\n",
       "             'model.layers.8.post_attention_layernorm': 8192,\n",
       "             'model.layers.8.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.9': 436224000,\n",
       "             'model.layers.9.self_attn': 83886080,\n",
       "             'model.layers.9.self_attn.q_proj': 33554432,\n",
       "             'model.layers.9.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.9.self_attn.k_proj': 8388608,\n",
       "             'model.layers.9.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.9.self_attn.v_proj': 8388608,\n",
       "             'model.layers.9.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.9.self_attn.o_proj': 33554432,\n",
       "             'model.layers.9.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.9.mlp': 352321536,\n",
       "             'model.layers.9.mlp.gate_proj': 117440512,\n",
       "             'model.layers.9.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.9.mlp.up_proj': 117440512,\n",
       "             'model.layers.9.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.9.mlp.down_proj': 117440512,\n",
       "             'model.layers.9.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.9.input_layernorm': 8192,\n",
       "             'model.layers.9.input_layernorm.weight': 8192,\n",
       "             'model.layers.9.post_attention_layernorm': 8192,\n",
       "             'model.layers.9.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.10': 436224000,\n",
       "             'model.layers.10.self_attn': 83886080,\n",
       "             'model.layers.10.self_attn.q_proj': 33554432,\n",
       "             'model.layers.10.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.10.self_attn.k_proj': 8388608,\n",
       "             'model.layers.10.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.10.self_attn.v_proj': 8388608,\n",
       "             'model.layers.10.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.10.self_attn.o_proj': 33554432,\n",
       "             'model.layers.10.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.10.mlp': 352321536,\n",
       "             'model.layers.10.mlp.gate_proj': 117440512,\n",
       "             'model.layers.10.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.10.mlp.up_proj': 117440512,\n",
       "             'model.layers.10.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.10.mlp.down_proj': 117440512,\n",
       "             'model.layers.10.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.10.input_layernorm': 8192,\n",
       "             'model.layers.10.input_layernorm.weight': 8192,\n",
       "             'model.layers.10.post_attention_layernorm': 8192,\n",
       "             'model.layers.10.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.11': 436224000,\n",
       "             'model.layers.11.self_attn': 83886080,\n",
       "             'model.layers.11.self_attn.q_proj': 33554432,\n",
       "             'model.layers.11.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.11.self_attn.k_proj': 8388608,\n",
       "             'model.layers.11.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.11.self_attn.v_proj': 8388608,\n",
       "             'model.layers.11.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.11.self_attn.o_proj': 33554432,\n",
       "             'model.layers.11.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.11.mlp': 352321536,\n",
       "             'model.layers.11.mlp.gate_proj': 117440512,\n",
       "             'model.layers.11.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.11.mlp.up_proj': 117440512,\n",
       "             'model.layers.11.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.11.mlp.down_proj': 117440512,\n",
       "             'model.layers.11.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.11.input_layernorm': 8192,\n",
       "             'model.layers.11.input_layernorm.weight': 8192,\n",
       "             'model.layers.11.post_attention_layernorm': 8192,\n",
       "             'model.layers.11.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.12': 436224000,\n",
       "             'model.layers.12.self_attn': 83886080,\n",
       "             'model.layers.12.self_attn.q_proj': 33554432,\n",
       "             'model.layers.12.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.12.self_attn.k_proj': 8388608,\n",
       "             'model.layers.12.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.12.self_attn.v_proj': 8388608,\n",
       "             'model.layers.12.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.12.self_attn.o_proj': 33554432,\n",
       "             'model.layers.12.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.12.mlp': 352321536,\n",
       "             'model.layers.12.mlp.gate_proj': 117440512,\n",
       "             'model.layers.12.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.12.mlp.up_proj': 117440512,\n",
       "             'model.layers.12.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.12.mlp.down_proj': 117440512,\n",
       "             'model.layers.12.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.12.input_layernorm': 8192,\n",
       "             'model.layers.12.input_layernorm.weight': 8192,\n",
       "             'model.layers.12.post_attention_layernorm': 8192,\n",
       "             'model.layers.12.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.13': 436224000,\n",
       "             'model.layers.13.self_attn': 83886080,\n",
       "             'model.layers.13.self_attn.q_proj': 33554432,\n",
       "             'model.layers.13.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.13.self_attn.k_proj': 8388608,\n",
       "             'model.layers.13.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.13.self_attn.v_proj': 8388608,\n",
       "             'model.layers.13.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.13.self_attn.o_proj': 33554432,\n",
       "             'model.layers.13.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.13.mlp': 352321536,\n",
       "             'model.layers.13.mlp.gate_proj': 117440512,\n",
       "             'model.layers.13.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.13.mlp.up_proj': 117440512,\n",
       "             'model.layers.13.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.13.mlp.down_proj': 117440512,\n",
       "             'model.layers.13.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.13.input_layernorm': 8192,\n",
       "             'model.layers.13.input_layernorm.weight': 8192,\n",
       "             'model.layers.13.post_attention_layernorm': 8192,\n",
       "             'model.layers.13.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.14': 436224000,\n",
       "             'model.layers.14.self_attn': 83886080,\n",
       "             'model.layers.14.self_attn.q_proj': 33554432,\n",
       "             'model.layers.14.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.14.self_attn.k_proj': 8388608,\n",
       "             'model.layers.14.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.14.self_attn.v_proj': 8388608,\n",
       "             'model.layers.14.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.14.self_attn.o_proj': 33554432,\n",
       "             'model.layers.14.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.14.mlp': 352321536,\n",
       "             'model.layers.14.mlp.gate_proj': 117440512,\n",
       "             'model.layers.14.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.14.mlp.up_proj': 117440512,\n",
       "             'model.layers.14.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.14.mlp.down_proj': 117440512,\n",
       "             'model.layers.14.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.14.input_layernorm': 8192,\n",
       "             'model.layers.14.input_layernorm.weight': 8192,\n",
       "             'model.layers.14.post_attention_layernorm': 8192,\n",
       "             'model.layers.14.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.15': 436224000,\n",
       "             'model.layers.15.self_attn': 83886080,\n",
       "             'model.layers.15.self_attn.q_proj': 33554432,\n",
       "             'model.layers.15.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.15.self_attn.k_proj': 8388608,\n",
       "             'model.layers.15.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.15.self_attn.v_proj': 8388608,\n",
       "             'model.layers.15.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.15.self_attn.o_proj': 33554432,\n",
       "             'model.layers.15.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.15.mlp': 352321536,\n",
       "             'model.layers.15.mlp.gate_proj': 117440512,\n",
       "             'model.layers.15.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.15.mlp.up_proj': 117440512,\n",
       "             'model.layers.15.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.15.mlp.down_proj': 117440512,\n",
       "             'model.layers.15.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.15.input_layernorm': 8192,\n",
       "             'model.layers.15.input_layernorm.weight': 8192,\n",
       "             'model.layers.15.post_attention_layernorm': 8192,\n",
       "             'model.layers.15.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.16': 436224000,\n",
       "             'model.layers.16.self_attn': 83886080,\n",
       "             'model.layers.16.self_attn.q_proj': 33554432,\n",
       "             'model.layers.16.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.16.self_attn.k_proj': 8388608,\n",
       "             'model.layers.16.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.16.self_attn.v_proj': 8388608,\n",
       "             'model.layers.16.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.16.self_attn.o_proj': 33554432,\n",
       "             'model.layers.16.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.16.mlp': 352321536,\n",
       "             'model.layers.16.mlp.gate_proj': 117440512,\n",
       "             'model.layers.16.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.16.mlp.up_proj': 117440512,\n",
       "             'model.layers.16.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.16.mlp.down_proj': 117440512,\n",
       "             'model.layers.16.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.16.input_layernorm': 8192,\n",
       "             'model.layers.16.input_layernorm.weight': 8192,\n",
       "             'model.layers.16.post_attention_layernorm': 8192,\n",
       "             'model.layers.16.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.17': 436224000,\n",
       "             'model.layers.17.self_attn': 83886080,\n",
       "             'model.layers.17.self_attn.q_proj': 33554432,\n",
       "             'model.layers.17.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.17.self_attn.k_proj': 8388608,\n",
       "             'model.layers.17.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.17.self_attn.v_proj': 8388608,\n",
       "             'model.layers.17.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.17.self_attn.o_proj': 33554432,\n",
       "             'model.layers.17.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.17.mlp': 352321536,\n",
       "             'model.layers.17.mlp.gate_proj': 117440512,\n",
       "             'model.layers.17.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.17.mlp.up_proj': 117440512,\n",
       "             'model.layers.17.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.17.mlp.down_proj': 117440512,\n",
       "             'model.layers.17.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.17.input_layernorm': 8192,\n",
       "             'model.layers.17.input_layernorm.weight': 8192,\n",
       "             'model.layers.17.post_attention_layernorm': 8192,\n",
       "             'model.layers.17.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.18': 436224000,\n",
       "             'model.layers.18.self_attn': 83886080,\n",
       "             'model.layers.18.self_attn.q_proj': 33554432,\n",
       "             'model.layers.18.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.18.self_attn.k_proj': 8388608,\n",
       "             'model.layers.18.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.18.self_attn.v_proj': 8388608,\n",
       "             'model.layers.18.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.18.self_attn.o_proj': 33554432,\n",
       "             'model.layers.18.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.18.mlp': 352321536,\n",
       "             'model.layers.18.mlp.gate_proj': 117440512,\n",
       "             'model.layers.18.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.18.mlp.up_proj': 117440512,\n",
       "             'model.layers.18.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.18.mlp.down_proj': 117440512,\n",
       "             'model.layers.18.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.18.input_layernorm': 8192,\n",
       "             'model.layers.18.input_layernorm.weight': 8192,\n",
       "             'model.layers.18.post_attention_layernorm': 8192,\n",
       "             'model.layers.18.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.19': 436224000,\n",
       "             'model.layers.19.self_attn': 83886080,\n",
       "             'model.layers.19.self_attn.q_proj': 33554432,\n",
       "             'model.layers.19.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.19.self_attn.k_proj': 8388608,\n",
       "             'model.layers.19.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.19.self_attn.v_proj': 8388608,\n",
       "             'model.layers.19.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.19.self_attn.o_proj': 33554432,\n",
       "             'model.layers.19.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.19.mlp': 352321536,\n",
       "             'model.layers.19.mlp.gate_proj': 117440512,\n",
       "             'model.layers.19.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.19.mlp.up_proj': 117440512,\n",
       "             'model.layers.19.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.19.mlp.down_proj': 117440512,\n",
       "             'model.layers.19.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.19.input_layernorm': 8192,\n",
       "             'model.layers.19.input_layernorm.weight': 8192,\n",
       "             'model.layers.19.post_attention_layernorm': 8192,\n",
       "             'model.layers.19.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.20': 436224000,\n",
       "             'model.layers.20.self_attn': 83886080,\n",
       "             'model.layers.20.self_attn.q_proj': 33554432,\n",
       "             'model.layers.20.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.20.self_attn.k_proj': 8388608,\n",
       "             'model.layers.20.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.20.self_attn.v_proj': 8388608,\n",
       "             'model.layers.20.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.20.self_attn.o_proj': 33554432,\n",
       "             'model.layers.20.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.20.mlp': 352321536,\n",
       "             'model.layers.20.mlp.gate_proj': 117440512,\n",
       "             'model.layers.20.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.20.mlp.up_proj': 117440512,\n",
       "             'model.layers.20.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.20.mlp.down_proj': 117440512,\n",
       "             'model.layers.20.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.20.input_layernorm': 8192,\n",
       "             'model.layers.20.input_layernorm.weight': 8192,\n",
       "             'model.layers.20.post_attention_layernorm': 8192,\n",
       "             'model.layers.20.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.21': 436224000,\n",
       "             'model.layers.21.self_attn': 83886080,\n",
       "             'model.layers.21.self_attn.q_proj': 33554432,\n",
       "             'model.layers.21.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.21.self_attn.k_proj': 8388608,\n",
       "             'model.layers.21.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.21.self_attn.v_proj': 8388608,\n",
       "             'model.layers.21.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.21.self_attn.o_proj': 33554432,\n",
       "             'model.layers.21.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.21.mlp': 352321536,\n",
       "             'model.layers.21.mlp.gate_proj': 117440512,\n",
       "             'model.layers.21.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.21.mlp.up_proj': 117440512,\n",
       "             'model.layers.21.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.21.mlp.down_proj': 117440512,\n",
       "             'model.layers.21.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.21.input_layernorm': 8192,\n",
       "             'model.layers.21.input_layernorm.weight': 8192,\n",
       "             'model.layers.21.post_attention_layernorm': 8192,\n",
       "             'model.layers.21.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.22': 436224000,\n",
       "             'model.layers.22.self_attn': 83886080,\n",
       "             'model.layers.22.self_attn.q_proj': 33554432,\n",
       "             'model.layers.22.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.22.self_attn.k_proj': 8388608,\n",
       "             'model.layers.22.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.22.self_attn.v_proj': 8388608,\n",
       "             'model.layers.22.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.22.self_attn.o_proj': 33554432,\n",
       "             'model.layers.22.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.22.mlp': 352321536,\n",
       "             'model.layers.22.mlp.gate_proj': 117440512,\n",
       "             'model.layers.22.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.22.mlp.up_proj': 117440512,\n",
       "             'model.layers.22.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.22.mlp.down_proj': 117440512,\n",
       "             'model.layers.22.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.22.input_layernorm': 8192,\n",
       "             'model.layers.22.input_layernorm.weight': 8192,\n",
       "             'model.layers.22.post_attention_layernorm': 8192,\n",
       "             'model.layers.22.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.23': 436224000,\n",
       "             'model.layers.23.self_attn': 83886080,\n",
       "             'model.layers.23.self_attn.q_proj': 33554432,\n",
       "             'model.layers.23.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.23.self_attn.k_proj': 8388608,\n",
       "             'model.layers.23.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.23.self_attn.v_proj': 8388608,\n",
       "             'model.layers.23.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.23.self_attn.o_proj': 33554432,\n",
       "             'model.layers.23.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.23.mlp': 352321536,\n",
       "             'model.layers.23.mlp.gate_proj': 117440512,\n",
       "             'model.layers.23.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.23.mlp.up_proj': 117440512,\n",
       "             'model.layers.23.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.23.mlp.down_proj': 117440512,\n",
       "             'model.layers.23.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.23.input_layernorm': 8192,\n",
       "             'model.layers.23.input_layernorm.weight': 8192,\n",
       "             'model.layers.23.post_attention_layernorm': 8192,\n",
       "             'model.layers.23.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.24': 436224000,\n",
       "             'model.layers.24.self_attn': 83886080,\n",
       "             'model.layers.24.self_attn.q_proj': 33554432,\n",
       "             'model.layers.24.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.24.self_attn.k_proj': 8388608,\n",
       "             'model.layers.24.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.24.self_attn.v_proj': 8388608,\n",
       "             'model.layers.24.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.24.self_attn.o_proj': 33554432,\n",
       "             'model.layers.24.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.24.mlp': 352321536,\n",
       "             'model.layers.24.mlp.gate_proj': 117440512,\n",
       "             'model.layers.24.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.24.mlp.up_proj': 117440512,\n",
       "             'model.layers.24.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.24.mlp.down_proj': 117440512,\n",
       "             'model.layers.24.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.24.input_layernorm': 8192,\n",
       "             'model.layers.24.input_layernorm.weight': 8192,\n",
       "             'model.layers.24.post_attention_layernorm': 8192,\n",
       "             'model.layers.24.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.25': 436224000,\n",
       "             'model.layers.25.self_attn': 83886080,\n",
       "             'model.layers.25.self_attn.q_proj': 33554432,\n",
       "             'model.layers.25.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.25.self_attn.k_proj': 8388608,\n",
       "             'model.layers.25.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.25.self_attn.v_proj': 8388608,\n",
       "             'model.layers.25.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.25.self_attn.o_proj': 33554432,\n",
       "             'model.layers.25.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.25.mlp': 352321536,\n",
       "             'model.layers.25.mlp.gate_proj': 117440512,\n",
       "             'model.layers.25.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.25.mlp.up_proj': 117440512,\n",
       "             'model.layers.25.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.25.mlp.down_proj': 117440512,\n",
       "             'model.layers.25.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.25.input_layernorm': 8192,\n",
       "             'model.layers.25.input_layernorm.weight': 8192,\n",
       "             'model.layers.25.post_attention_layernorm': 8192,\n",
       "             'model.layers.25.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.26': 436224000,\n",
       "             'model.layers.26.self_attn': 83886080,\n",
       "             'model.layers.26.self_attn.q_proj': 33554432,\n",
       "             'model.layers.26.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.26.self_attn.k_proj': 8388608,\n",
       "             'model.layers.26.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.26.self_attn.v_proj': 8388608,\n",
       "             'model.layers.26.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.26.self_attn.o_proj': 33554432,\n",
       "             'model.layers.26.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.26.mlp': 352321536,\n",
       "             'model.layers.26.mlp.gate_proj': 117440512,\n",
       "             'model.layers.26.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.26.mlp.up_proj': 117440512,\n",
       "             'model.layers.26.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.26.mlp.down_proj': 117440512,\n",
       "             'model.layers.26.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.26.input_layernorm': 8192,\n",
       "             'model.layers.26.input_layernorm.weight': 8192,\n",
       "             'model.layers.26.post_attention_layernorm': 8192,\n",
       "             'model.layers.26.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.27': 436224000,\n",
       "             'model.layers.27.self_attn': 83886080,\n",
       "             'model.layers.27.self_attn.q_proj': 33554432,\n",
       "             'model.layers.27.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.27.self_attn.k_proj': 8388608,\n",
       "             'model.layers.27.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.27.self_attn.v_proj': 8388608,\n",
       "             'model.layers.27.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.27.self_attn.o_proj': 33554432,\n",
       "             'model.layers.27.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.27.mlp': 352321536,\n",
       "             'model.layers.27.mlp.gate_proj': 117440512,\n",
       "             'model.layers.27.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.27.mlp.up_proj': 117440512,\n",
       "             'model.layers.27.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.27.mlp.down_proj': 117440512,\n",
       "             'model.layers.27.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.27.input_layernorm': 8192,\n",
       "             'model.layers.27.input_layernorm.weight': 8192,\n",
       "             'model.layers.27.post_attention_layernorm': 8192,\n",
       "             'model.layers.27.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.28': 436224000,\n",
       "             'model.layers.28.self_attn': 83886080,\n",
       "             'model.layers.28.self_attn.q_proj': 33554432,\n",
       "             'model.layers.28.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.28.self_attn.k_proj': 8388608,\n",
       "             'model.layers.28.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.28.self_attn.v_proj': 8388608,\n",
       "             'model.layers.28.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.28.self_attn.o_proj': 33554432,\n",
       "             'model.layers.28.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.28.mlp': 352321536,\n",
       "             'model.layers.28.mlp.gate_proj': 117440512,\n",
       "             'model.layers.28.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.28.mlp.up_proj': 117440512,\n",
       "             'model.layers.28.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.28.mlp.down_proj': 117440512,\n",
       "             'model.layers.28.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.28.input_layernorm': 8192,\n",
       "             'model.layers.28.input_layernorm.weight': 8192,\n",
       "             'model.layers.28.post_attention_layernorm': 8192,\n",
       "             'model.layers.28.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.29': 436224000,\n",
       "             'model.layers.29.self_attn': 83886080,\n",
       "             'model.layers.29.self_attn.q_proj': 33554432,\n",
       "             'model.layers.29.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.29.self_attn.k_proj': 8388608,\n",
       "             'model.layers.29.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.29.self_attn.v_proj': 8388608,\n",
       "             'model.layers.29.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.29.self_attn.o_proj': 33554432,\n",
       "             'model.layers.29.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.29.mlp': 352321536,\n",
       "             'model.layers.29.mlp.gate_proj': 117440512,\n",
       "             'model.layers.29.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.29.mlp.up_proj': 117440512,\n",
       "             'model.layers.29.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.29.mlp.down_proj': 117440512,\n",
       "             'model.layers.29.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.29.input_layernorm': 8192,\n",
       "             'model.layers.29.input_layernorm.weight': 8192,\n",
       "             'model.layers.29.post_attention_layernorm': 8192,\n",
       "             'model.layers.29.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.30': 436224000,\n",
       "             'model.layers.30.self_attn': 83886080,\n",
       "             'model.layers.30.self_attn.q_proj': 33554432,\n",
       "             'model.layers.30.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.30.self_attn.k_proj': 8388608,\n",
       "             'model.layers.30.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.30.self_attn.v_proj': 8388608,\n",
       "             'model.layers.30.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.30.self_attn.o_proj': 33554432,\n",
       "             'model.layers.30.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.30.mlp': 352321536,\n",
       "             'model.layers.30.mlp.gate_proj': 117440512,\n",
       "             'model.layers.30.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.30.mlp.up_proj': 117440512,\n",
       "             'model.layers.30.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.30.mlp.down_proj': 117440512,\n",
       "             'model.layers.30.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.30.input_layernorm': 8192,\n",
       "             'model.layers.30.input_layernorm.weight': 8192,\n",
       "             'model.layers.30.post_attention_layernorm': 8192,\n",
       "             'model.layers.30.post_attention_layernorm.weight': 8192,\n",
       "             'model.layers.31': 436224000,\n",
       "             'model.layers.31.self_attn': 83886080,\n",
       "             'model.layers.31.self_attn.q_proj': 33554432,\n",
       "             'model.layers.31.self_attn.q_proj.weight': 33554432,\n",
       "             'model.layers.31.self_attn.k_proj': 8388608,\n",
       "             'model.layers.31.self_attn.k_proj.weight': 8388608,\n",
       "             'model.layers.31.self_attn.v_proj': 8388608,\n",
       "             'model.layers.31.self_attn.v_proj.weight': 8388608,\n",
       "             'model.layers.31.self_attn.o_proj': 33554432,\n",
       "             'model.layers.31.self_attn.o_proj.weight': 33554432,\n",
       "             'model.layers.31.mlp': 352321536,\n",
       "             'model.layers.31.mlp.gate_proj': 117440512,\n",
       "             'model.layers.31.mlp.gate_proj.weight': 117440512,\n",
       "             'model.layers.31.mlp.up_proj': 117440512,\n",
       "             'model.layers.31.mlp.up_proj.weight': 117440512,\n",
       "             'model.layers.31.mlp.down_proj': 117440512,\n",
       "             'model.layers.31.mlp.down_proj.weight': 117440512,\n",
       "             'model.layers.31.input_layernorm': 8192,\n",
       "             'model.layers.31.input_layernorm.weight': 8192,\n",
       "             'model.layers.31.post_attention_layernorm': 8192,\n",
       "             'model.layers.31.post_attention_layernorm.weight': 8192,\n",
       "             'model.norm': 8192,\n",
       "             'model.norm.weight': 8192,\n",
       "             'lm_head': 1050673152,\n",
       "             'lm_head.weight': 1050673152,\n",
       "             'model.rotary_emb': 256,\n",
       "             'model.rotary_emb.inv_freq': 256})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from quantization_theory_helper import compute_module_sizes\n",
    "module_sizes = compute_module_sizes(model)\n",
    "module_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ddc458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 16.060522752 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model size is {module_sizes[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad077d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0121, -0.0148, -0.0086,  ...,  0.0090, -0.0033,  0.0128],\n",
      "        [ 0.0237, -0.0069, -0.0052,  ...,  0.0018,  0.0193,  0.0024],\n",
      "        [ 0.0099, -0.0110, -0.0004,  ..., -0.0042,  0.0024, -0.0179],\n",
      "        ...,\n",
      "        [ 0.0113, -0.0243,  0.0244,  ..., -0.0208,  0.0085, -0.0139],\n",
      "        [-0.0157, -0.0093, -0.0126,  ...,  0.0101,  0.0010, -0.0160],\n",
      "        [-0.0038,  0.0068,  0.0087,  ..., -0.0063, -0.0054, -0.0116]],\n",
      "       dtype=torch.bfloat16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers[0].mlp.down_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0de77e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before quantization: Parameter containing:\n",
      "tensor([[ 0.0121, -0.0148, -0.0086,  ...,  0.0090, -0.0033,  0.0128],\n",
      "        [ 0.0237, -0.0069, -0.0052,  ...,  0.0018,  0.0193,  0.0024],\n",
      "        [ 0.0099, -0.0110, -0.0004,  ..., -0.0042,  0.0024, -0.0179],\n",
      "        ...,\n",
      "        [ 0.0113, -0.0243,  0.0244,  ..., -0.0208,  0.0085, -0.0139],\n",
      "        [-0.0157, -0.0093, -0.0126,  ...,  0.0101,  0.0010, -0.0160],\n",
      "        [-0.0038,  0.0068,  0.0087,  ..., -0.0063, -0.0054, -0.0116]],\n",
      "       dtype=torch.bfloat16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from quanto import quantize, freeze, qint8\n",
    "\n",
    "# Use quanto's qtype objects directly\n",
    "print(f\"Before quantization: {model.model.layers[0].mlp.down_proj.weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d5e8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize(model, weights=qint8, activations=None)\n",
    "# print(model)\n",
    "# print(f\"After quantization: {model.model.layers[0].mlp.down_proj.weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a822ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b822bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): QLinear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): QLinear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): QLinear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): QLinear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): QLinear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): QLinear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): QLinear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): QLinear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81b13a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QBytesTensor(tensor([[ 25, -31, -18,  ...,  19,  -7,  27],\n",
      "        [ 68, -20, -15,  ...,   5,  55,   7],\n",
      "        [ 26, -28,  -1,  ..., -11,   6, -46],\n",
      "        ...,\n",
      "        [ 15, -33,  33,  ..., -28,  12, -19],\n",
      "        [-38, -22, -30,  ...,  24,   2, -38],\n",
      "        [-10,  17,  22,  ..., -16, -13, -29]], dtype=torch.int8), scale=tensor([[0.0005],\n",
      "        [0.0003],\n",
      "        [0.0004],\n",
      "        ...,\n",
      "        [0.0007],\n",
      "        [0.0004],\n",
      "        [0.0004]], dtype=torch.bfloat16), dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers[0].mlp.down_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "036a58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quantized model size is 8.558875144 GB\n"
     ]
    }
   ],
   "source": [
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"The quantized model size is {module_sizes[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a682112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Move inputs to the same device as the model\u001b[39;00m\n\u001b[32m      2\u001b[39m inputs = {k: v.to(model.device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2829\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2827\u001b[39m     probs = nn.functional.softmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m   2828\u001b[39m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2829\u001b[39m     next_tokens = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m   2830\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2831\u001b[39m     next_tokens = torch.argmax(next_token_scores, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "# Move inputs to the same device as the model\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb234b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
