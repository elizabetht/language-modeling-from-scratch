{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e270624",
   "metadata": {
    "papermill": {
     "duration": 0.001732,
     "end_time": "2026-01-22T02:00:36.334752",
     "exception": false,
     "start_time": "2026-01-22T02:00:36.333020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "Install the necessary packages for quantization-aware training including ipywidgets for notebook UI, nvidia-modelopt for quantization tools, and trl for transformer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657febd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:00:36.339216Z",
     "iopub.status.busy": "2026-01-22T02:00:36.339071Z",
     "iopub.status.idle": "2026-01-22T02:00:36.920647Z",
     "shell.execute_reply": "2026-01-22T02:00:36.919630Z"
    },
    "papermill": {
     "duration": 0.585709,
     "end_time": "2026-01-22T02:00:36.921838",
     "exception": false,
     "start_time": "2026-01-22T02:00:36.336129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: ipywidgets in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (8.1.8)\r\n",
      "Requirement already satisfied: trl in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (0.22.2)\r\n",
      "Requirement already satisfied: nvidia-modelopt[all] in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (0.40.0)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (9.9.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\r\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\r\n",
      "Requirement already satisfied: ninja in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.13.0)\r\n",
      "Requirement already satisfied: numpy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2.4.1)\r\n",
      "Requirement already satisfied: packaging in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (25.0)\r\n",
      "Requirement already satisfied: pydantic>=2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2.12.5)\r\n",
      "Requirement already satisfied: nvidia-ml-py>=12 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (13.590.44)\r\n",
      "Requirement already satisfied: rich in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (14.2.0)\r\n",
      "Requirement already satisfied: scipy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.17.0)\r\n",
      "Requirement already satisfied: tqdm in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (4.67.1)\r\n",
      "Requirement already satisfied: pulp in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (3.3.0)\r\n",
      "Requirement already satisfied: regex in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2026.1.15)\r\n",
      "Requirement already satisfied: safetensors in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.7.0)\r\n",
      "Requirement already satisfied: torch>=2.6 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2.9.0+cu130)\r\n",
      "Requirement already satisfied: torchprofile>=0.0.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.0.4)\r\n",
      "Requirement already satisfied: cppimport in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (22.8.2)\r\n",
      "Requirement already satisfied: ml_dtypes in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.5.4)\r\n",
      "Requirement already satisfied: onnx-graphsurgeon in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.5.8)\r\n",
      "Requirement already satisfied: onnx~=1.19.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.19.1)\r\n",
      "Requirement already satisfied: onnxconverter-common~=1.16.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.16.0)\r\n",
      "Requirement already satisfied: onnxruntime~=1.22.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.22.1)\r\n",
      "Requirement already satisfied: onnxscript in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.5.7)\r\n",
      "Requirement already satisfied: onnxslim>=0.1.76 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.1.82)\r\n",
      "Requirement already satisfied: polygraphy>=0.49.22 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.49.26)\r\n",
      "Requirement already satisfied: accelerate>=1.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.12.0)\r\n",
      "Requirement already satisfied: datasets>=3.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (4.3.0)\r\n",
      "Requirement already satisfied: diffusers>=0.32.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.36.0)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.24.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.36.0)\r\n",
      "Requirement already satisfied: peft>=0.17.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.18.1)\r\n",
      "Requirement already satisfied: transformers<5.0,>=4.53 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (4.55.4)\r\n",
      "Requirement already satisfied: deepspeed>=0.9.6 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.18.4)\r\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnx~=1.19.0->nvidia-modelopt[all]) (6.33.4)\r\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnx~=1.19.0->nvidia-modelopt[all]) (4.15.0)\r\n",
      "Requirement already satisfied: coloredlogs in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxruntime~=1.22.0->nvidia-modelopt[all]) (15.0.1)\r\n",
      "Requirement already satisfied: flatbuffers in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxruntime~=1.22.0->nvidia-modelopt[all]) (25.12.19)\r\n",
      "Requirement already satisfied: sympy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxruntime~=1.22.0->nvidia-modelopt[all]) (1.14.0)\r\n",
      "Requirement already satisfied: filelock in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (3.20.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (6.0.3)\r\n",
      "Requirement already satisfied: requests in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (2.32.5)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (0.21.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (2025.9.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (1.2.0)\r\n",
      "Requirement already satisfied: psutil in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from accelerate>=1.0.0->nvidia-modelopt[all]) (7.2.1)\r\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (22.0.0)\r\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (0.4.0)\r\n",
      "Requirement already satisfied: pandas in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (2.3.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx<1.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (0.28.1)\r\n",
      "Requirement already satisfied: xxhash in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (3.6.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (3.13.3)\r\n",
      "Requirement already satisfied: anyio in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (4.12.1)\r\n",
      "Requirement already satisfied: certifi in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (2026.1.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.0.9)\r\n",
      "Requirement already satisfied: idna in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (3.11)\r\n",
      "Requirement already satisfied: h11>=0.16 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (0.16.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (6.7.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (0.4.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.22.0)\r\n",
      "Requirement already satisfied: einops in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (0.8.1)\r\n",
      "Requirement already satisfied: hjson in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (3.1.0)\r\n",
      "Requirement already satisfied: msgpack in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (1.1.2)\r\n",
      "Requirement already satisfied: py-cpuinfo in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (9.0.0)\r\n",
      "Requirement already satisfied: importlib_metadata in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from diffusers>=0.32.2->nvidia-modelopt[all]) (8.7.1)\r\n",
      "Requirement already satisfied: Pillow in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from diffusers>=0.32.2->nvidia-modelopt[all]) (12.1.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\r\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.18.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\r\n",
      "Requirement already satisfied: pygments>=2.11.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\r\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\r\n",
      "Requirement already satisfied: wcwidth in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\r\n",
      "Requirement already satisfied: colorama in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxslim>=0.1.76->nvidia-modelopt[all]) (0.4.6)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt[all]) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt[all]) (2.41.5)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt[all]) (0.4.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from requests->transformers<5.0,>=4.53->nvidia-modelopt[all]) (3.4.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from requests->transformers<5.0,>=4.53->nvidia-modelopt[all]) (2.6.3)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\r\n",
      "Requirement already satisfied: pure-eval in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from sympy->onnxruntime~=1.22.0->nvidia-modelopt[all]) (1.3.0)\r\n",
      "Requirement already satisfied: setuptools in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (80.9.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.6.1)\r\n",
      "Requirement already satisfied: jinja2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (9.13.0.50)\r\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.0.19)\r\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (12.0.0.15)\r\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (10.4.0.35)\r\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (12.0.3.29)\r\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (12.6.2.49)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (0.8.0)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (2.27.7)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.3.24)\r\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.39)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.39)\r\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (1.15.0.42)\r\n",
      "Requirement already satisfied: triton==3.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.5.0)\r\n",
      "Requirement already satisfied: torchvision>=0.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torchprofile>=0.0.4->nvidia-modelopt[all]) (0.24.0)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime~=1.22.0->nvidia-modelopt[all]) (10.0)\r\n",
      "Requirement already satisfied: mako in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from cppimport->nvidia-modelopt[all]) (1.3.10)\r\n",
      "Requirement already satisfied: pybind11 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from cppimport->nvidia-modelopt[all]) (3.0.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=3.20 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from importlib_metadata->diffusers>=0.32.2->nvidia-modelopt[all]) (3.23.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.6->nvidia-modelopt[all]) (3.0.3)\r\n",
      "Requirement already satisfied: onnx_ir<2,>=0.1.12 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxscript->nvidia-modelopt[all]) (0.1.14)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->nvidia-modelopt[all]) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->nvidia-modelopt[all]) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->nvidia-modelopt[all]) (2025.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->nvidia-modelopt[all]) (1.17.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from rich->nvidia-modelopt[all]) (4.0.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->nvidia-modelopt[all]) (0.1.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets nvidia-modelopt[all] trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5ef1f",
   "metadata": {
    "papermill": {
     "duration": 0.001421,
     "end_time": "2026-01-22T02:00:36.925213",
     "exception": false,
     "start_time": "2026-01-22T02:00:36.923792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Environment Variables and Configure Model\n",
    "\n",
    "Load Hugging Face authentication token from environment and specify the base model to use for quantization-aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2edf1f7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:00:36.929206Z",
     "iopub.status.busy": "2026-01-22T02:00:36.929065Z",
     "iopub.status.idle": "2026-01-22T02:00:36.933940Z",
     "shell.execute_reply": "2026-01-22T02:00:36.933411Z"
    },
    "papermill": {
     "duration": 0.008158,
     "end_time": "2026-01-22T02:00:36.934731",
     "exception": false,
     "start_time": "2026-01-22T02:00:36.926573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_WRITE_TOKEN\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81efff24",
   "metadata": {
    "papermill": {
     "duration": 0.001323,
     "end_time": "2026-01-22T02:00:36.937426",
     "exception": false,
     "start_time": "2026-01-22T02:00:36.936103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enable Hugging Face Checkpointing\n",
    "\n",
    "Enable ModelOpt's Hugging Face checkpointing feature to ensure quantized models can be properly saved and loaded with Hugging Face's transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d53c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:00:36.941049Z",
     "iopub.status.busy": "2026-01-22T02:00:36.940995Z",
     "iopub.status.idle": "2026-01-22T02:00:41.124249Z",
     "shell.execute_reply": "2026-01-22T02:00:41.124036Z"
    },
    "papermill": {
     "duration": 4.187328,
     "end_time": "2026-01-22T02:00:41.126112",
     "exception": false,
     "start_time": "2026-01-22T02:00:36.938784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu130 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelOpt save/restore enabled for `transformers` library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelOpt save/restore enabled for `diffusers` library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelOpt save/restore enabled for `peft` library.\n"
     ]
    }
   ],
   "source": [
    "import modelopt.torch.opt as mto\n",
    "mto.enable_huggingface_checkpointing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a15e72",
   "metadata": {
    "papermill": {
     "duration": 0.002982,
     "end_time": "2026-01-22T02:00:41.132566",
     "exception": false,
     "start_time": "2026-01-22T02:00:41.129584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configure Model Arguments\n",
    "\n",
    "Define model configuration including attention implementation, precision (bfloat16), and device settings. Setting `use_cache=False` is required for training to enable gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95899a36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:00:41.139754Z",
     "iopub.status.busy": "2026-01-22T02:00:41.139478Z",
     "iopub.status.idle": "2026-01-22T02:00:41.141994Z",
     "shell.execute_reply": "2026-01-22T02:00:41.141408Z"
    },
    "papermill": {
     "duration": 0.007423,
     "end_time": "2026-01-22T02:00:41.142929",
     "exception": false,
     "start_time": "2026-01-22T02:00:41.135506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import ModelConfig\n",
    "\n",
    "model_args = ModelConfig(\n",
    "    model_name_or_path=model_name,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model_kwargs = {\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "    \"attn_implementation\": model_args.attn_implementation,\n",
    "    \"torch_dtype\": model_args.torch_dtype,\n",
    "    \"use_cache\": False,\n",
    "    \"device_map\": \"auto\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3b189",
   "metadata": {
    "papermill": {
     "duration": 0.002923,
     "end_time": "2026-01-22T02:00:41.148887",
     "exception": false,
     "start_time": "2026-01-22T02:00:41.145964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Load the pre-trained Llama model and its corresponding tokenizer from Hugging Face Hub using the configurations defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0891b3ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:00:41.155856Z",
     "iopub.status.busy": "2026-01-22T02:00:41.155744Z",
     "iopub.status.idle": "2026-01-22T02:02:04.364286Z",
     "shell.execute_reply": "2026-01-22T02:02:04.363459Z"
    },
    "papermill": {
     "duration": 83.213478,
     "end_time": "2026-01-22T02:02:04.365274",
     "exception": false,
     "start_time": "2026-01-22T02:00:41.151796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e70cec58ab849e3a9215b87f100b0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953a4a4",
   "metadata": {
    "papermill": {
     "duration": 0.003587,
     "end_time": "2026-01-22T02:02:04.372505",
     "exception": false,
     "start_time": "2026-01-22T02:02:04.368918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configure Dataset Arguments\n",
    "\n",
    "Specify the dataset to use for training (Multilingual-Thinking dataset) and define the train/test split configuration with 10% held out for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1097745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:02:04.379905Z",
     "iopub.status.busy": "2026-01-22T02:02:04.379786Z",
     "iopub.status.idle": "2026-01-22T02:02:04.383660Z",
     "shell.execute_reply": "2026-01-22T02:02:04.383322Z"
    },
    "papermill": {
     "duration": 0.008645,
     "end_time": "2026-01-22T02:02:04.384273",
     "exception": false,
     "start_time": "2026-01-22T02:02:04.375628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import ScriptArguments\n",
    "\n",
    "script_args = ScriptArguments(\n",
    "    dataset_name=\"HuggingFaceH4/Multilingual-Thinking\",\n",
    "    dataset_train_split=\"train\",\n",
    "    dataset_test_split=\"test\",\n",
    ")\n",
    "test_size = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bc0f6",
   "metadata": {
    "papermill": {
     "duration": 0.003143,
     "end_time": "2026-01-22T02:02:04.390542",
     "exception": false,
     "start_time": "2026-01-22T02:02:04.387399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load and Split Dataset\n",
    "\n",
    "Load the multilingual reasoning dataset and split it into training and evaluation sets using a fixed seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e407c9ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:02:04.397698Z",
     "iopub.status.busy": "2026-01-22T02:02:04.397594Z",
     "iopub.status.idle": "2026-01-22T02:02:05.304946Z",
     "shell.execute_reply": "2026-01-22T02:02:05.304337Z"
    },
    "papermill": {
     "duration": 0.912042,
     "end_time": "2026-01-22T02:02:05.305628",
     "exception": false,
     "start_time": "2026-01-22T02:02:04.393586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(script_args.dataset_name)\n",
    "# split the dataset into train and test\n",
    "dataset = dataset[script_args.dataset_train_split].train_test_split(test_size=test_size, seed=42)\n",
    "train_dataset = dataset[script_args.dataset_train_split]\n",
    "eval_dataset = dataset[script_args.dataset_test_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028492e",
   "metadata": {
    "papermill": {
     "duration": 0.003282,
     "end_time": "2026-01-22T02:02:05.312573",
     "exception": false,
     "start_time": "2026-01-22T02:02:05.309291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configure Training Arguments\n",
    "\n",
    "Set up training hyperparameters including learning rate, batch size, evaluation strategy, and checkpointing configuration. The training will run for 1 epoch with evaluation every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d24ca34d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:02:05.320020Z",
     "iopub.status.busy": "2026-01-22T02:02:05.319913Z",
     "iopub.status.idle": "2026-01-22T02:02:05.342003Z",
     "shell.execute_reply": "2026-01-22T02:02:05.341521Z"
    },
    "papermill": {
     "duration": 0.026856,
     "end_time": "2026-01-22T02:02:05.342627",
     "exception": false,
     "start_time": "2026-01-22T02:02:05.315771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_length=4096,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_on_start=True,\n",
    "    logging_steps=25,\n",
    "    save_steps=450,\n",
    "    eval_steps=50,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5354e1",
   "metadata": {
    "papermill": {
     "duration": 0.003161,
     "end_time": "2026-01-22T02:02:05.349031",
     "exception": false,
     "start_time": "2026-01-22T02:02:05.345870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize SFT Trainer\n",
    "\n",
    "Create a Supervised Fine-Tuning (SFT) trainer that will handle the training loop, evaluation, and model optimization during quantization-aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93fb06be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:02:05.356091Z",
     "iopub.status.busy": "2026-01-22T02:02:05.356027Z",
     "iopub.status.idle": "2026-01-22T02:02:05.560983Z",
     "shell.execute_reply": "2026-01-22T02:02:05.560209Z"
    },
    "papermill": {
     "duration": 0.209869,
     "end_time": "2026-01-22T02:02:05.562033",
     "exception": false,
     "start_time": "2026-01-22T02:02:05.352164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[script_args.dataset_train_split],\n",
    "    eval_dataset=dataset[script_args.dataset_test_split],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc3bb9",
   "metadata": {
    "papermill": {
     "duration": 0.003273,
     "end_time": "2026-01-22T02:02:05.569049",
     "exception": false,
     "start_time": "2026-01-22T02:02:05.565776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup Quantization Configuration\n",
    "\n",
    "Configure quantization settings (NVFP4 format) and prepare calibration data. The forward loop function is defined to run calibration samples through the model for quantization parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a6b531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:02:05.576192Z",
     "iopub.status.busy": "2026-01-22T02:02:05.576124Z",
     "iopub.status.idle": "2026-01-22T02:02:05.578544Z",
     "shell.execute_reply": "2026-01-22T02:02:05.577989Z"
    },
    "papermill": {
     "duration": 0.006988,
     "end_time": "2026-01-22T02:02:05.579207",
     "exception": false,
     "start_time": "2026-01-22T02:02:05.572219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import modelopt.torch.quantization as mtq\n",
    "\n",
    "# Some configs don't need calibration, but other quantization configurations may require it.\n",
    "quantization_config = mtq.FP8_DEFAULT_CFG\n",
    "calib_size = 512\n",
    "\n",
    "dataset = torch.utils.data.Subset(\n",
    "    trainer.eval_dataset, list(range(min(len(trainer.eval_dataset), calib_size)))\n",
    ")\n",
    "data_loader = trainer.get_eval_dataloader(dataset)\n",
    "\n",
    "\n",
    "def forward_loop(model):\n",
    "    for data in data_loader:\n",
    "        model(**data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a5770",
   "metadata": {
    "papermill": {
     "duration": 0.003186,
     "end_time": "2026-01-22T02:02:05.585564",
     "exception": false,
     "start_time": "2026-01-22T02:02:05.582378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Apply Quantization to Model\n",
    "\n",
    "Apply the quantization configuration to the model using the calibration data. This prepares the model for quantization-aware training by inserting quantization layers and running calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b6925f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:02:05.592990Z",
     "iopub.status.busy": "2026-01-22T02:02:05.592862Z",
     "iopub.status.idle": "2026-01-22T02:03:11.204576Z",
     "shell.execute_reply": "2026-01-22T02:03:11.204209Z"
    },
    "papermill": {
     "duration": 65.622187,
     "end_time": "2026-01-22T02:03:11.210943",
     "exception": false,
     "start_time": "2026-01-22T02:02:05.588756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered <class 'transformers.models.llama.modeling_llama.LlamaAttention'> to _QuantAttention for KV Cache quantization\n",
      "Inserted 771 quantizers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.7461 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6992 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0610 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4180 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4629 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=3.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=3.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1963 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6133 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=6.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5352 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=6.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3555 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=6.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0967 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.8711 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4141 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=4.3438 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5273 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=4.3438 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4629 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=478.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2617 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3496 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0586 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=1.3594 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3457 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=5.7188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3242 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=5.7188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2480 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.9219 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6055 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=7.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2188 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=7.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2832 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=7.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1357 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=1.2578 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3770 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=6.7812 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5078 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=6.7812 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2754 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=3.0938 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5469 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3301 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3359 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0742 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=1.1406 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3574 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3047 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=4.5938 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6562 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=10.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2578 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=10.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5273 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=10.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1357 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=1.7266 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3477 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4180 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4277 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=5.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6016 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=11.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2734 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=11.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2871 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=11.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0596 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.7344 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6953 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=10.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4668 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=10.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3008 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=5.2188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6602 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=11.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2539 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=11.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2676 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=11.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0781 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=1.6953 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3184 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4102 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2871 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=5.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6406 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=13.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4531 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=13.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4551 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=13.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0854 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.0312 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3262 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2930 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=4.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.7344 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2490 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3691 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=1.9766 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3574 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4805 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3047 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=5.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3789 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4375 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0645 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=1.8047 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5156 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3457 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2461 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=7.3438 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2021 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3125 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0723 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2773 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2314 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=5.4688 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5938 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2520 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2305 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.1094 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4590 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4258 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=5.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.9141 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1777 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0996 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.1406 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.7773 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4844 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3652 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=5.1562 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.9102 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3359 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3730 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0879 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.3594 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2207 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4883 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4570 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=7.7812 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.8203 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4297 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4004 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1416 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.3281 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4668 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3809 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2812 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6328 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4023 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3633 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1367 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=1.9453 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4824 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4863 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2676 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.8086 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3613 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0776 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=1.6719 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4688 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3984 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3164 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=10.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6172 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2139 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2832 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0869 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.8281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=7.6875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5156 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=7.6875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2363 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.6875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.7891 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2656 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3027 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1025 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.6406 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5703 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4688 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2793 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=12.4375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.8203 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2402 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3164 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.0859 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=1.9766 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6094 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4004 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2852 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=13.4375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4570 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2246 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3379 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1240 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=2.9688 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2949 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5469 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5430 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=19.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.8281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2285 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3418 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=18.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1074 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=3.7188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3301 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3965 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1895 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5898 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2324 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4707 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1230 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=3.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4512 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1953 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=21.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6367 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2412 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=16.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1191 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=3.7031 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2988 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5234 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3535 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=23.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2314 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4336 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1348 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=3.6719 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3887 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3672 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=8.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2344 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=23.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2324 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3535 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1992 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=3.9844 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3398 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4238 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3320 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=34.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6992 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2246 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4082 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=15.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2578 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=3.9062 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4844 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=10.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4121 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=10.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2148 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=35.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5312 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (28): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2793 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3008 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.1699 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=4.0312 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6211 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=10.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4902 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=10.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=36.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6680 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (29): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=12.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3086 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=12.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4004 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=12.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2539 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=5.0312 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3535 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=12.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4727 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=12.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=44.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.7344 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (30): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2637 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3594 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=4.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.2832 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5859 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=38.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4336 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (31): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4199 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5703 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.3125 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=6.9688 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.4375 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.7695 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=14.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.6719 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=141.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((4, 3) bit fake per-tensor amax=0.5469 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): QuantLinear(\n",
       "    in_features=4096, out_features=128256, bias=False\n",
       "    (input_quantizer): TensorQuantizer(disabled)\n",
       "    (output_quantizer): TensorQuantizer(disabled)\n",
       "    (weight_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtq.quantize(model, quantization_config, forward_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74e934",
   "metadata": {
    "papermill": {
     "duration": 0.0043,
     "end_time": "2026-01-22T02:03:11.219877",
     "exception": false,
     "start_time": "2026-01-22T02:03:11.215577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Start Quantization-Aware Training\n",
    "\n",
    "Configure CUDA environment variables for proper compilation and start the training process. The model will learn while simulating the effects of quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fdcb43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T02:03:11.230163Z",
     "iopub.status.busy": "2026-01-22T02:03:11.230031Z",
     "iopub.status.idle": "2026-01-22T05:23:35.075545Z",
     "shell.execute_reply": "2026-01-22T05:23:35.075262Z"
    },
    "papermill": {
     "duration": 12023.854429,
     "end_time": "2026-01-22T05:23:35.078577",
     "exception": false,
     "start_time": "2026-01-22T02:03:11.224148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extension modelopt_cuda_ext_fp8...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded extension modelopt_cuda_ext_fp8 in 0.0 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 3:19:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.256705</td>\n",
       "      <td>1.331495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.574359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.120400</td>\n",
       "      <td>1.029272</td>\n",
       "      <td>1.007731</td>\n",
       "      <td>51429.000000</td>\n",
       "      <td>0.717016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.949200</td>\n",
       "      <td>0.999120</td>\n",
       "      <td>1.062640</td>\n",
       "      <td>110540.000000</td>\n",
       "      <td>0.720615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.123900</td>\n",
       "      <td>1.075860</td>\n",
       "      <td>1.037810</td>\n",
       "      <td>165574.000000</td>\n",
       "      <td>0.707689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.256000</td>\n",
       "      <td>1.233319</td>\n",
       "      <td>1.116227</td>\n",
       "      <td>220373.000000</td>\n",
       "      <td>0.684657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.352100</td>\n",
       "      <td>1.339444</td>\n",
       "      <td>1.291183</td>\n",
       "      <td>272024.000000</td>\n",
       "      <td>0.666098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.514800</td>\n",
       "      <td>1.350784</td>\n",
       "      <td>1.153571</td>\n",
       "      <td>328039.000000</td>\n",
       "      <td>0.668970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.384000</td>\n",
       "      <td>1.373163</td>\n",
       "      <td>1.329398</td>\n",
       "      <td>386991.000000</td>\n",
       "      <td>0.661902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.407600</td>\n",
       "      <td>1.355927</td>\n",
       "      <td>1.248261</td>\n",
       "      <td>441345.000000</td>\n",
       "      <td>0.666697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.544000</td>\n",
       "      <td>1.319960</td>\n",
       "      <td>1.399849</td>\n",
       "      <td>499140.000000</td>\n",
       "      <td>0.669181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>1.447886</td>\n",
       "      <td>0.903966</td>\n",
       "      <td>544539.000000</td>\n",
       "      <td>0.664320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.767700</td>\n",
       "      <td>1.421973</td>\n",
       "      <td>0.899712</td>\n",
       "      <td>599860.000000</td>\n",
       "      <td>0.668319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>1.361334</td>\n",
       "      <td>1.051329</td>\n",
       "      <td>656645.000000</td>\n",
       "      <td>0.673029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.863000</td>\n",
       "      <td>1.353766</td>\n",
       "      <td>0.965657</td>\n",
       "      <td>717102.000000</td>\n",
       "      <td>0.674436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.884600</td>\n",
       "      <td>1.370498</td>\n",
       "      <td>0.953080</td>\n",
       "      <td>767833.000000</td>\n",
       "      <td>0.674566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>1.342496</td>\n",
       "      <td>0.890185</td>\n",
       "      <td>826279.000000</td>\n",
       "      <td>0.681601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>1.323553</td>\n",
       "      <td>0.892315</td>\n",
       "      <td>882453.000000</td>\n",
       "      <td>0.684524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.771100</td>\n",
       "      <td>1.269831</td>\n",
       "      <td>1.096359</td>\n",
       "      <td>936070.000000</td>\n",
       "      <td>0.686874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.724700</td>\n",
       "      <td>1.284513</td>\n",
       "      <td>0.864136</td>\n",
       "      <td>998280.000000</td>\n",
       "      <td>0.693645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.852700</td>\n",
       "      <td>1.435809</td>\n",
       "      <td>0.673576</td>\n",
       "      <td>1059309.000000</td>\n",
       "      <td>0.686255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.291800</td>\n",
       "      <td>1.392679</td>\n",
       "      <td>0.678886</td>\n",
       "      <td>1118717.000000</td>\n",
       "      <td>0.689826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.306800</td>\n",
       "      <td>1.379394</td>\n",
       "      <td>0.746161</td>\n",
       "      <td>1171561.000000</td>\n",
       "      <td>0.688917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.332100</td>\n",
       "      <td>1.369874</td>\n",
       "      <td>0.693194</td>\n",
       "      <td>1222354.000000</td>\n",
       "      <td>0.692867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.293800</td>\n",
       "      <td>1.379769</td>\n",
       "      <td>0.669918</td>\n",
       "      <td>1276161.000000</td>\n",
       "      <td>0.695932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.301700</td>\n",
       "      <td>1.382653</td>\n",
       "      <td>0.649263</td>\n",
       "      <td>1328009.000000</td>\n",
       "      <td>0.696328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>1.397535</td>\n",
       "      <td>0.606371</td>\n",
       "      <td>1380127.000000</td>\n",
       "      <td>0.695147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.282700</td>\n",
       "      <td>1.388932</td>\n",
       "      <td>0.601667</td>\n",
       "      <td>1439247.000000</td>\n",
       "      <td>0.699890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>1.377709</td>\n",
       "      <td>0.607109</td>\n",
       "      <td>1497420.000000</td>\n",
       "      <td>0.700098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>1.486400</td>\n",
       "      <td>0.533172</td>\n",
       "      <td>1549788.000000</td>\n",
       "      <td>0.699514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>1.474488</td>\n",
       "      <td>0.518522</td>\n",
       "      <td>1604930.000000</td>\n",
       "      <td>0.702063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>1.480134</td>\n",
       "      <td>0.514877</td>\n",
       "      <td>1658881.000000</td>\n",
       "      <td>0.702648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>1.520920</td>\n",
       "      <td>0.475090</td>\n",
       "      <td>1713826.000000</td>\n",
       "      <td>0.702955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>1.483075</td>\n",
       "      <td>0.500669</td>\n",
       "      <td>1768088.000000</td>\n",
       "      <td>0.705474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>1.468338</td>\n",
       "      <td>0.501459</td>\n",
       "      <td>1830422.000000</td>\n",
       "      <td>0.706291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>1.474658</td>\n",
       "      <td>0.491373</td>\n",
       "      <td>1886982.000000</td>\n",
       "      <td>0.706125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>1.476064</td>\n",
       "      <td>0.489839</td>\n",
       "      <td>1939492.000000</td>\n",
       "      <td>0.706192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>1.480390</td>\n",
       "      <td>0.488799</td>\n",
       "      <td>1996560.000000</td>\n",
       "      <td>0.705520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>1.608372</td>\n",
       "      <td>0.431233</td>\n",
       "      <td>2053855.000000</td>\n",
       "      <td>0.703521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>1.604736</td>\n",
       "      <td>0.435123</td>\n",
       "      <td>2104640.000000</td>\n",
       "      <td>0.703244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>1.614527</td>\n",
       "      <td>0.428050</td>\n",
       "      <td>2156729.000000</td>\n",
       "      <td>0.703819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>1.619416</td>\n",
       "      <td>0.426414</td>\n",
       "      <td>2214314.000000</td>\n",
       "      <td>0.703216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>1.626971</td>\n",
       "      <td>0.422544</td>\n",
       "      <td>2270922.000000</td>\n",
       "      <td>0.704202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>1.624551</td>\n",
       "      <td>0.422819</td>\n",
       "      <td>2328311.000000</td>\n",
       "      <td>0.704605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>1.622002</td>\n",
       "      <td>0.424154</td>\n",
       "      <td>2386390.000000</td>\n",
       "      <td>0.704248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>1.622436</td>\n",
       "      <td>0.423935</td>\n",
       "      <td>2442102.000000</td>\n",
       "      <td>0.703553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>1.622485</td>\n",
       "      <td>0.424342</td>\n",
       "      <td>2495700.000000</td>\n",
       "      <td>0.704627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to output/checkpoint-450/modelopt_state.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to output/checkpoint-900/modelopt_state.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to output/checkpoint-1350/modelopt_state.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to output/checkpoint-1800/modelopt_state.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to output/checkpoint-2250/modelopt_state.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=0.5248833482530382, metrics={'train_runtime': 12023.6405, 'train_samples_per_second': 0.374, 'train_steps_per_second': 0.187, 'total_flos': 1.123802430234624e+17, 'train_loss': 0.5248833482530382, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"12.1\"\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "os.environ[\"PATH\"] = \"/usr/local/cuda/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6924e66b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T05:23:35.085184Z",
     "iopub.status.busy": "2026-01-22T05:23:35.085108Z",
     "iopub.status.idle": "2026-01-22T05:23:52.743090Z",
     "shell.execute_reply": "2026-01-22T05:23:52.742201Z"
    },
    "papermill": {
     "duration": 17.662785,
     "end_time": "2026-01-22T05:23:52.744208",
     "exception": false,
     "start_time": "2026-01-22T05:23:35.081423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/modelopt/torch/export/unified_export_hf.py:413: UserWarning: Model's original dtype (bfloat16) differs from target dtype (torch.bfloat16), which may lead to numerical errors.\n",
      "  warnings.warn(\n",
      "/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/modelopt/torch/export/unified_export_hf.py:269: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weight_scaling_factor = torch.tensor(weight_quantizer.amax / weight_quantizer.maxbound)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./output/fp8/tokenizer_config.json',\n",
       " './output/fp8/special_tokens_map.json',\n",
       " './output/fp8/chat_template.jinja',\n",
       " './output/fp8/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from modelopt.torch.export import export_hf_checkpoint\n",
    "\n",
    "export_path = \"./output/fp8\"\n",
    "export_hf_checkpoint(model, dtype=torch.bfloat16, export_dir=export_path)\n",
    "tokenizer.save_pretrained(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48d1c1bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T05:23:52.832046Z",
     "iopub.status.busy": "2026-01-22T05:23:52.831918Z",
     "iopub.status.idle": "2026-01-22T05:23:52.836367Z",
     "shell.execute_reply": "2026-01-22T05:23:52.835273Z"
    },
    "papermill": {
     "duration": 0.050461,
     "end_time": "2026-01-22T05:23:52.837077",
     "exception": false,
     "start_time": "2026-01-22T05:23:52.786616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved config\n",
      " Created model card\n",
      "\n",
      "Model saved to ./output/fp8/\n",
      "Contents: ['special_tokens_map.json', 'README.md', 'model-00002-of-00002.safetensors', 'tokenizer_config.json', 'chat_template.jinja', 'hf_quant_config.json', 'generation_config.json', 'config.json', 'model-00001-of-00002.safetensors', 'model.safetensors.index.json', 'tokenizer.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save model config\n",
    "model.config.save_pretrained(export_path)\n",
    "print(\" Saved config\")\n",
    "\n",
    "# Create a README model card\n",
    "model_card = \"\"\"---\n",
    "license: llama3.1\n",
    "base_model: meta-llama/Llama-3.1-8B-Instruct\n",
    "tags:\n",
    "  - llama\n",
    "  - quantized\n",
    "  - nvidia-modeloptimizer\n",
    "  - FP8\n",
    "  - QAT \n",
    "library_name: nvidia-modeloptimizer\n",
    "---\n",
    "\n",
    "# Llama-3.1-8B-Instruct Quantized (ModelOpt FP8) through QAT\n",
    "\n",
    "This is a quantized version of [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) using [modelopt](https://github.com/NVIDIA/Model-Optimizer) with NVFP4 weight quantization.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model:** meta-llama/Llama-3.1-8B-Instruct\n",
    "- **Quantization Method:** modelopt FP8 Quantization Aware Training (QAT) \n",
    "- **Weight Precision:** FP8\n",
    "- **Original Size:** ~16 GB (bfloat16)\n",
    "- **Quantized Size:** ~6 GB (fp8)\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load base model structure\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-FP8\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Load tokenizer and generate\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4-QAT\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "This model inherits the [Llama 3.1 Community License](https://llama.meta.com/llama3_1/license/).\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{export_path}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "print(\" Created model card\")\n",
    "\n",
    "print(f\"\\nModel saved to {export_path}/\")\n",
    "print(\"Contents:\", os.listdir(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6810dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T05:23:53.128438Z",
     "iopub.status.busy": "2026-01-22T05:23:53.128308Z",
     "iopub.status.idle": "2026-01-22T05:27:09.189823Z",
     "shell.execute_reply": "2026-01-22T05:27:09.187600Z"
    },
    "papermill": {
     "duration": 196.256296,
     "end_time": "2026-01-22T05:27:09.190815",
     "exception": false,
     "start_time": "2026-01-22T05:23:52.934519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Repository created: tokenlabsdotrun/Llama-3.1-8B-ModelOpt-FP8-QAT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921c47c218c748d8989ab63413e38b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e46aeb723148d79e31430116f56889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Uploaded to https://huggingface.co/tokenlabsdotrun/Llama-3.1-8B-ModelOpt-FP8-QAT\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "# Load write token from .env file\n",
    "hf_write_token = os.getenv(\"HF_WRITE_TOKEN\")\n",
    "\n",
    "# Upload to HuggingFace Hub\n",
    "repo_name = \"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-FP8-QAT\"  # Change to your username/repo\n",
    "\n",
    "try:\n",
    "    # Create the repo (set private=True if you want it private)\n",
    "    create_repo(repo_name, exist_ok=True, private=False, token=hf_write_token)\n",
    "    print(f\" Repository created: {repo_name}\")\n",
    "    \n",
    "    # Upload all files\n",
    "    upload_folder(\n",
    "        folder_path=export_path,\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload Llama-3.1-8B quantized with ModelOpt FP8-QAT\",\n",
    "        token=hf_write_token\n",
    "    )\n",
    "    print(f\" Uploaded to https://huggingface.co/{repo_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7c6c818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T05:27:09.198033Z",
     "iopub.status.busy": "2026-01-22T05:27:09.197917Z",
     "iopub.status.idle": "2026-01-22T05:27:09.314988Z",
     "shell.execute_reply": "2026-01-22T05:27:09.313830Z"
    },
    "papermill": {
     "duration": 0.121962,
     "end_time": "2026-01-22T05:27:09.316244",
     "exception": false,
     "start_time": "2026-01-22T05:27:09.194282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 9.08120448 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")  # Add parent directory to path\n",
    "\n",
    "# Force reload the module to pick up changes\n",
    "import importlib\n",
    "import quantization_theory_helper\n",
    "importlib.reload(quantization_theory_helper)\n",
    "\n",
    "from quantization_theory_helper import compute_module_sizes\n",
    "module_size = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_size[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc02887",
   "metadata": {
    "papermill": {
     "duration": 0.002772,
     "end_time": "2026-01-22T05:27:09.322323",
     "exception": false,
     "start_time": "2026-01-22T05:27:09.319551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12397.068814,
   "end_time": "2026-01-22T05:27:12.787215",
   "environment_variables": {},
   "exception": null,
   "input_path": "FP8_QAT.ipynb",
   "output_path": "output_executed_fp8_qat.ipynb",
   "parameters": {},
   "start_time": "2026-01-22T02:00:35.718401",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0ee44864bb474e1d96d68a1f02d30ed5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_56e08a5fe48b4b91921b5611cfec5b9d",
       "placeholder": "",
       "style": "IPY_MODEL_19995c30a606429db62e0ac9557a52c8",
       "tabbable": null,
       "tooltip": null,
       "value": "4/4[01:21&lt;00:00,17.71s/it]"
      }
     },
     "13a14496ae454f458be67c305dbd5837": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "14813f0942584c2895e578ff54507001": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "16272829562d4789ae7c44448ec704b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "183f86f20f844f5a89a41e1d53606bad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19995c30a606429db62e0ac9557a52c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2511303660b449d881d268a1d8726dd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b289ad3293cc4977a3305f9db036805e",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ada0d0ac14a0445eb4be634ea65d3b02",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "2be04578112e41d5a244f710e3016b39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "32c1d3e7a70b40bbb73adaf297401c97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b50fcf0bcf0435f978cc159eef20685": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ddd5afb5a7c2406a9a0c80288e061a63",
       "placeholder": "",
       "style": "IPY_MODEL_16272829562d4789ae7c44448ec704b5",
       "tabbable": null,
       "tooltip": null,
       "value": "NewDataUpload:100%"
      }
     },
     "50b4f31b5fdf4e5488ba4831aa887290": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "56e08a5fe48b4b91921b5611cfec5b9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e5b4539085d403291e7fc74a2f66d25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "638b8f50ebd84b1aad32d2d0393af9df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "72e46aeb723148d79e31430116f56889": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3b50fcf0bcf0435f978cc159eef20685",
        "IPY_MODEL_a0fbc3a5e3964210aa5bffe445730df9",
        "IPY_MODEL_deca68404e184be8b70166e3fc8cd13d"
       ],
       "layout": "IPY_MODEL_183f86f20f844f5a89a41e1d53606bad",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8974841224504fb6b6bbd92566d4433d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b36338460d684fe3ad3c9952dadff816",
       "placeholder": "",
       "style": "IPY_MODEL_a95601d77d8349ad971d194300d5bdc0",
       "tabbable": null,
       "tooltip": null,
       "value": "ProcessingFiles(3/3):100%"
      }
     },
     "8e70cec58ab849e3a9215b87f100b0c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ee86520a9ca44de2a79455dede5e2e7c",
        "IPY_MODEL_b4560ff9efab4d9aad33b464d29886b7",
        "IPY_MODEL_0ee44864bb474e1d96d68a1f02d30ed5"
       ],
       "layout": "IPY_MODEL_5e5b4539085d403291e7fc74a2f66d25",
       "tabbable": null,
       "tooltip": null
      }
     },
     "921c47c218c748d8989ab63413e38b88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8974841224504fb6b6bbd92566d4433d",
        "IPY_MODEL_2511303660b449d881d268a1d8726dd7",
        "IPY_MODEL_f9731a4befa34878a807993f0a87f390"
       ],
       "layout": "IPY_MODEL_da063c586fa441deb200006e0486d56f",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a0fbc3a5e3964210aa5bffe445730df9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bfbb21af383b465c849a9afd48db44c7",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2be04578112e41d5a244f710e3016b39",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "a95601d77d8349ad971d194300d5bdc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ada0d0ac14a0445eb4be634ea65d3b02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b289ad3293cc4977a3305f9db036805e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "b36338460d684fe3ad3c9952dadff816": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4560ff9efab4d9aad33b464d29886b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_32c1d3e7a70b40bbb73adaf297401c97",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_14813f0942584c2895e578ff54507001",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "bca1d3ca49aa41458f00cea052163ae1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bfbb21af383b465c849a9afd48db44c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "da063c586fa441deb200006e0486d56f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ddd5afb5a7c2406a9a0c80288e061a63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "deca68404e184be8b70166e3fc8cd13d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fb86702d2d3c426d8eeeacd5e0e14ebe",
       "placeholder": "",
       "style": "IPY_MODEL_13a14496ae454f458be67c305dbd5837",
       "tabbable": null,
       "tooltip": null,
       "value": "8.83GB/8.83GB,36.3MB/s"
      }
     },
     "ee86520a9ca44de2a79455dede5e2e7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_50b4f31b5fdf4e5488ba4831aa887290",
       "placeholder": "",
       "style": "IPY_MODEL_638b8f50ebd84b1aad32d2d0393af9df",
       "tabbable": null,
       "tooltip": null,
       "value": "Loadingcheckpointshards:100%"
      }
     },
     "f75e3426b1294310944cf9a17abf5b19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f9731a4befa34878a807993f0a87f390": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bca1d3ca49aa41458f00cea052163ae1",
       "placeholder": "",
       "style": "IPY_MODEL_f75e3426b1294310944cf9a17abf5b19",
       "tabbable": null,
       "tooltip": null,
       "value": "9.10GB/9.10GB,36.3MB/s"
      }
     },
     "fb86702d2d3c426d8eeeacd5e0e14ebe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}