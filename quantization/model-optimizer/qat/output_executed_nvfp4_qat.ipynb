{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e270624",
   "metadata": {
    "papermill": {
     "duration": 0.001776,
     "end_time": "2026-01-21T05:21:34.658101",
     "exception": false,
     "start_time": "2026-01-21T05:21:34.656325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "Install the necessary packages for quantization-aware training including ipywidgets for notebook UI, nvidia-modelopt for quantization tools, and trl for transformer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657febd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:21:34.661662Z",
     "iopub.status.busy": "2026-01-21T05:21:34.661529Z",
     "iopub.status.idle": "2026-01-21T05:21:35.208909Z",
     "shell.execute_reply": "2026-01-21T05:21:35.208166Z"
    },
    "papermill": {
     "duration": 0.550139,
     "end_time": "2026-01-21T05:21:35.209638",
     "exception": false,
     "start_time": "2026-01-21T05:21:34.659499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: ipywidgets in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (8.1.8)\r\n",
      "Requirement already satisfied: trl in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (0.22.2)\r\n",
      "Requirement already satisfied: nvidia-modelopt[all] in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (0.40.0)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (9.9.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\r\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\r\n",
      "Requirement already satisfied: ninja in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.13.0)\r\n",
      "Requirement already satisfied: numpy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2.4.1)\r\n",
      "Requirement already satisfied: packaging in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (25.0)\r\n",
      "Requirement already satisfied: pydantic>=2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2.12.5)\r\n",
      "Requirement already satisfied: nvidia-ml-py>=12 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (13.590.44)\r\n",
      "Requirement already satisfied: rich in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (14.2.0)\r\n",
      "Requirement already satisfied: scipy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.17.0)\r\n",
      "Requirement already satisfied: tqdm in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (4.67.1)\r\n",
      "Requirement already satisfied: pulp in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (3.3.0)\r\n",
      "Requirement already satisfied: regex in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2026.1.15)\r\n",
      "Requirement already satisfied: safetensors in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.7.0)\r\n",
      "Requirement already satisfied: torch>=2.6 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2.9.0+cu130)\r\n",
      "Requirement already satisfied: torchprofile>=0.0.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.0.4)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cppimport in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (22.8.2)\r\n",
      "Requirement already satisfied: ml_dtypes in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.5.4)\r\n",
      "Requirement already satisfied: onnx-graphsurgeon in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.5.8)\r\n",
      "Requirement already satisfied: onnx~=1.19.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.19.1)\r\n",
      "Requirement already satisfied: onnxconverter-common~=1.16.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.16.0)\r\n",
      "Requirement already satisfied: onnxruntime~=1.22.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.22.1)\r\n",
      "Requirement already satisfied: onnxscript in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.5.7)\r\n",
      "Requirement already satisfied: onnxslim>=0.1.76 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.1.82)\r\n",
      "Requirement already satisfied: polygraphy>=0.49.22 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.49.26)\r\n",
      "Requirement already satisfied: accelerate>=1.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.12.0)\r\n",
      "Requirement already satisfied: datasets>=3.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (4.3.0)\r\n",
      "Requirement already satisfied: diffusers>=0.32.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.36.0)\r\n",
      "Requirement already satisfied: huggingface_hub>=0.24.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.36.0)\r\n",
      "Requirement already satisfied: peft>=0.17.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.18.1)\r\n",
      "Requirement already satisfied: transformers<5.0,>=4.53 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (4.55.4)\r\n",
      "Requirement already satisfied: deepspeed>=0.9.6 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.18.4)\r\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnx~=1.19.0->nvidia-modelopt[all]) (6.33.4)\r\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnx~=1.19.0->nvidia-modelopt[all]) (4.15.0)\r\n",
      "Requirement already satisfied: coloredlogs in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxruntime~=1.22.0->nvidia-modelopt[all]) (15.0.1)\r\n",
      "Requirement already satisfied: flatbuffers in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxruntime~=1.22.0->nvidia-modelopt[all]) (25.12.19)\r\n",
      "Requirement already satisfied: sympy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxruntime~=1.22.0->nvidia-modelopt[all]) (1.14.0)\r\n",
      "Requirement already satisfied: filelock in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (3.20.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (6.0.3)\r\n",
      "Requirement already satisfied: requests in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (2.32.5)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (0.21.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (2025.9.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (1.2.0)\r\n",
      "Requirement already satisfied: psutil in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from accelerate>=1.0.0->nvidia-modelopt[all]) (7.2.1)\r\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (22.0.0)\r\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (0.4.0)\r\n",
      "Requirement already satisfied: pandas in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (2.3.3)\r\n",
      "Requirement already satisfied: httpx<1.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (0.28.1)\r\n",
      "Requirement already satisfied: xxhash in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (3.6.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (3.13.3)\r\n",
      "Requirement already satisfied: anyio in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (4.12.1)\r\n",
      "Requirement already satisfied: certifi in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (2026.1.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.0.9)\r\n",
      "Requirement already satisfied: idna in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (3.11)\r\n",
      "Requirement already satisfied: h11>=0.16 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (0.16.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (6.7.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (0.4.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.22.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (0.8.1)\r\n",
      "Requirement already satisfied: hjson in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (3.1.0)\r\n",
      "Requirement already satisfied: msgpack in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (1.1.2)\r\n",
      "Requirement already satisfied: py-cpuinfo in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (9.0.0)\r\n",
      "Requirement already satisfied: importlib_metadata in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from diffusers>=0.32.2->nvidia-modelopt[all]) (8.7.1)\r\n",
      "Requirement already satisfied: Pillow in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from diffusers>=0.32.2->nvidia-modelopt[all]) (12.1.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\r\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.18.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\r\n",
      "Requirement already satisfied: pygments>=2.11.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\r\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\r\n",
      "Requirement already satisfied: wcwidth in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\r\n",
      "Requirement already satisfied: colorama in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxslim>=0.1.76->nvidia-modelopt[all]) (0.4.6)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt[all]) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt[all]) (2.41.5)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt[all]) (0.4.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from requests->transformers<5.0,>=4.53->nvidia-modelopt[all]) (3.4.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from requests->transformers<5.0,>=4.53->nvidia-modelopt[all]) (2.6.3)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\r\n",
      "Requirement already satisfied: pure-eval in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from sympy->onnxruntime~=1.22.0->nvidia-modelopt[all]) (1.3.0)\r\n",
      "Requirement already satisfied: setuptools in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (80.9.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.6.1)\r\n",
      "Requirement already satisfied: jinja2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.48)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (9.13.0.50)\r\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.0.19)\r\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (12.0.0.15)\r\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (10.4.0.35)\r\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (12.0.3.29)\r\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (12.6.2.49)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (0.8.0)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (2.27.7)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.3.24)\r\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.39)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.39)\r\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (1.15.0.42)\r\n",
      "Requirement already satisfied: triton==3.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.5.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision>=0.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torchprofile>=0.0.4->nvidia-modelopt[all]) (0.24.0)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime~=1.22.0->nvidia-modelopt[all]) (10.0)\r\n",
      "Requirement already satisfied: mako in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from cppimport->nvidia-modelopt[all]) (1.3.10)\r\n",
      "Requirement already satisfied: pybind11 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from cppimport->nvidia-modelopt[all]) (3.0.1)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from importlib_metadata->diffusers>=0.32.2->nvidia-modelopt[all]) (3.23.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.6->nvidia-modelopt[all]) (3.0.3)\r\n",
      "Requirement already satisfied: onnx_ir<2,>=0.1.12 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxscript->nvidia-modelopt[all]) (0.1.14)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->nvidia-modelopt[all]) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->nvidia-modelopt[all]) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->nvidia-modelopt[all]) (2025.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->nvidia-modelopt[all]) (1.17.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from rich->nvidia-modelopt[all]) (4.0.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->nvidia-modelopt[all]) (0.1.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets nvidia-modelopt[all] trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5ef1f",
   "metadata": {
    "papermill": {
     "duration": 0.002745,
     "end_time": "2026-01-21T05:21:35.215796",
     "exception": false,
     "start_time": "2026-01-21T05:21:35.213051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Environment Variables and Configure Model\n",
    "\n",
    "Load Hugging Face authentication token from environment and specify the base model to use for quantization-aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2edf1f7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:21:35.222877Z",
     "iopub.status.busy": "2026-01-21T05:21:35.222654Z",
     "iopub.status.idle": "2026-01-21T05:21:35.229453Z",
     "shell.execute_reply": "2026-01-21T05:21:35.228773Z"
    },
    "papermill": {
     "duration": 0.011555,
     "end_time": "2026-01-21T05:21:35.230136",
     "exception": false,
     "start_time": "2026-01-21T05:21:35.218581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_WRITE_TOKEN\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81efff24",
   "metadata": {
    "papermill": {
     "duration": 0.002667,
     "end_time": "2026-01-21T05:21:35.235590",
     "exception": false,
     "start_time": "2026-01-21T05:21:35.232923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enable Hugging Face Checkpointing\n",
    "\n",
    "Enable ModelOpt's Hugging Face checkpointing feature to ensure quantized models can be properly saved and loaded with Hugging Face's transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d53c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:21:35.242512Z",
     "iopub.status.busy": "2026-01-21T05:21:35.242304Z",
     "iopub.status.idle": "2026-01-21T05:21:38.916581Z",
     "shell.execute_reply": "2026-01-21T05:21:38.915977Z"
    },
    "papermill": {
     "duration": 3.679115,
     "end_time": "2026-01-21T05:21:38.917485",
     "exception": false,
     "start_time": "2026-01-21T05:21:35.238370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu130 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelOpt save/restore enabled for `transformers` library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelOpt save/restore enabled for `diffusers` library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelOpt save/restore enabled for `peft` library.\n"
     ]
    }
   ],
   "source": [
    "import modelopt.torch.opt as mto\n",
    "mto.enable_huggingface_checkpointing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a15e72",
   "metadata": {
    "papermill": {
     "duration": 0.002994,
     "end_time": "2026-01-21T05:21:38.923965",
     "exception": false,
     "start_time": "2026-01-21T05:21:38.920971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configure Model Arguments\n",
    "\n",
    "Define model configuration including attention implementation, precision (bfloat16), and device settings. Setting `use_cache=False` is required for training to enable gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95899a36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:21:38.931171Z",
     "iopub.status.busy": "2026-01-21T05:21:38.930905Z",
     "iopub.status.idle": "2026-01-21T05:21:38.933864Z",
     "shell.execute_reply": "2026-01-21T05:21:38.933140Z"
    },
    "papermill": {
     "duration": 0.007461,
     "end_time": "2026-01-21T05:21:38.934355",
     "exception": false,
     "start_time": "2026-01-21T05:21:38.926894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import ModelConfig\n",
    "\n",
    "model_args = ModelConfig(\n",
    "    model_name_or_path=model_name,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model_kwargs = {\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "    \"attn_implementation\": model_args.attn_implementation,\n",
    "    \"torch_dtype\": model_args.torch_dtype,\n",
    "    \"use_cache\": False,\n",
    "    \"device_map\": \"auto\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3b189",
   "metadata": {
    "papermill": {
     "duration": 0.00287,
     "end_time": "2026-01-21T05:21:38.940252",
     "exception": false,
     "start_time": "2026-01-21T05:21:38.937382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Load the pre-trained Llama model and its corresponding tokenizer from Hugging Face Hub using the configurations defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0891b3ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:21:38.947105Z",
     "iopub.status.busy": "2026-01-21T05:21:38.947003Z",
     "iopub.status.idle": "2026-01-21T05:23:01.898726Z",
     "shell.execute_reply": "2026-01-21T05:23:01.896978Z"
    },
    "papermill": {
     "duration": 82.956889,
     "end_time": "2026-01-21T05:23:01.900092",
     "exception": false,
     "start_time": "2026-01-21T05:21:38.943203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d80d98b4db4c68a037eb32a7be6a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953a4a4",
   "metadata": {
    "papermill": {
     "duration": 0.003483,
     "end_time": "2026-01-21T05:23:01.907239",
     "exception": false,
     "start_time": "2026-01-21T05:23:01.903756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configure Dataset Arguments\n",
    "\n",
    "Specify the dataset to use for training (Multilingual-Thinking dataset) and define the train/test split configuration with 10% held out for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1097745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:23:01.914427Z",
     "iopub.status.busy": "2026-01-21T05:23:01.914308Z",
     "iopub.status.idle": "2026-01-21T05:23:01.918039Z",
     "shell.execute_reply": "2026-01-21T05:23:01.917556Z"
    },
    "papermill": {
     "duration": 0.008362,
     "end_time": "2026-01-21T05:23:01.918647",
     "exception": false,
     "start_time": "2026-01-21T05:23:01.910285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import ScriptArguments\n",
    "\n",
    "script_args = ScriptArguments(\n",
    "    dataset_name=\"HuggingFaceH4/Multilingual-Thinking\",\n",
    "    dataset_train_split=\"train\",\n",
    "    dataset_test_split=\"test\",\n",
    ")\n",
    "test_size = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bc0f6",
   "metadata": {
    "papermill": {
     "duration": 0.003145,
     "end_time": "2026-01-21T05:23:01.924910",
     "exception": false,
     "start_time": "2026-01-21T05:23:01.921765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load and Split Dataset\n",
    "\n",
    "Load the multilingual reasoning dataset and split it into training and evaluation sets using a fixed seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e407c9ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:23:01.932847Z",
     "iopub.status.busy": "2026-01-21T05:23:01.932725Z",
     "iopub.status.idle": "2026-01-21T05:23:02.789174Z",
     "shell.execute_reply": "2026-01-21T05:23:02.788373Z"
    },
    "papermill": {
     "duration": 0.861966,
     "end_time": "2026-01-21T05:23:02.789956",
     "exception": false,
     "start_time": "2026-01-21T05:23:01.927990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(script_args.dataset_name)\n",
    "# split the dataset into train and test\n",
    "dataset = dataset[script_args.dataset_train_split].train_test_split(test_size=test_size, seed=42)\n",
    "train_dataset = dataset[script_args.dataset_train_split]\n",
    "eval_dataset = dataset[script_args.dataset_test_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028492e",
   "metadata": {
    "papermill": {
     "duration": 0.003173,
     "end_time": "2026-01-21T05:23:02.796730",
     "exception": false,
     "start_time": "2026-01-21T05:23:02.793557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configure Training Arguments\n",
    "\n",
    "Set up training hyperparameters including learning rate, batch size, evaluation strategy, and checkpointing configuration. The training will run for 1 epoch with evaluation every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e837692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:23:02.804052Z",
     "iopub.status.busy": "2026-01-21T05:23:02.803905Z",
     "iopub.status.idle": "2026-01-21T05:23:02.826874Z",
     "shell.execute_reply": "2026-01-21T05:23:02.826001Z"
    },
    "papermill": {
     "duration": 0.027864,
     "end_time": "2026-01-21T05:23:02.827669",
     "exception": false,
     "start_time": "2026-01-21T05:23:02.799805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_length=4096,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_on_start=True,\n",
    "    logging_steps=25,\n",
    "    save_steps=450,\n",
    "    eval_steps=50,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5354e1",
   "metadata": {
    "papermill": {
     "duration": 0.003273,
     "end_time": "2026-01-21T05:23:02.834339",
     "exception": false,
     "start_time": "2026-01-21T05:23:02.831066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize SFT Trainer\n",
    "\n",
    "Create a Supervised Fine-Tuning (SFT) trainer that will handle the training loop, evaluation, and model optimization during quantization-aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93fb06be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:23:02.841758Z",
     "iopub.status.busy": "2026-01-21T05:23:02.841596Z",
     "iopub.status.idle": "2026-01-21T05:23:03.078501Z",
     "shell.execute_reply": "2026-01-21T05:23:03.078158Z"
    },
    "papermill": {
     "duration": 0.242901,
     "end_time": "2026-01-21T05:23:03.080412",
     "exception": false,
     "start_time": "2026-01-21T05:23:02.837511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[script_args.dataset_train_split],\n",
    "    eval_dataset=dataset[script_args.dataset_test_split],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc3bb9",
   "metadata": {
    "papermill": {
     "duration": 0.003203,
     "end_time": "2026-01-21T05:23:03.087354",
     "exception": false,
     "start_time": "2026-01-21T05:23:03.084151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup Quantization Configuration\n",
    "\n",
    "Configure quantization settings (NVFP4 format) and prepare calibration data. The forward loop function is defined to run calibration samples through the model for quantization parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a6b531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:23:03.094500Z",
     "iopub.status.busy": "2026-01-21T05:23:03.094435Z",
     "iopub.status.idle": "2026-01-21T05:23:03.097007Z",
     "shell.execute_reply": "2026-01-21T05:23:03.096656Z"
    },
    "papermill": {
     "duration": 0.0073,
     "end_time": "2026-01-21T05:23:03.097782",
     "exception": false,
     "start_time": "2026-01-21T05:23:03.090482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import modelopt.torch.quantization as mtq\n",
    "\n",
    "# Some configs don't need calibration, but other quantization configurations may require it.\n",
    "quantization_config = mtq.NVFP4_DEFAULT_CFG\n",
    "calib_size = 512\n",
    "\n",
    "dataset = torch.utils.data.Subset(\n",
    "    trainer.eval_dataset, list(range(min(len(trainer.eval_dataset), calib_size)))\n",
    ")\n",
    "data_loader = trainer.get_eval_dataloader(dataset)\n",
    "\n",
    "\n",
    "def forward_loop(model):\n",
    "    for data in data_loader:\n",
    "        model(**data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a5770",
   "metadata": {
    "papermill": {
     "duration": 0.00324,
     "end_time": "2026-01-21T05:23:03.104224",
     "exception": false,
     "start_time": "2026-01-21T05:23:03.100984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Apply Quantization to Model\n",
    "\n",
    "Apply the quantization configuration to the model using the calibration data. This prepares the model for quantization-aware training by inserting quantization layers and running calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b6925f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:23:03.112384Z",
     "iopub.status.busy": "2026-01-21T05:23:03.112255Z",
     "iopub.status.idle": "2026-01-21T05:24:06.801512Z",
     "shell.execute_reply": "2026-01-21T05:24:06.800910Z"
    },
    "papermill": {
     "duration": 63.700588,
     "end_time": "2026-01-21T05:24:06.807956",
     "exception": false,
     "start_time": "2026-01-21T05:23:03.107368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered <class 'transformers.models.llama.modeling_llama.LlamaAttention'> to _QuantAttention for KV Cache quantization\n",
      "Inserted 771 quantizers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7461 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6992 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0610 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4180 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4629 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1963 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6133 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5352 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3555 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0967 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8711 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4141 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.3438 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5273 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.3438 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4629 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=478.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2617 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3496 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0586 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.3594 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3457 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.7188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3242 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.7188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2480 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.9219 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6055 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2188 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2832 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1357 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.2578 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3770 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.7812 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5078 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.7812 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2754 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.0938 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5469 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3301 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3359 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0742 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.1406 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3574 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3047 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.5938 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6562 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2578 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5273 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1357 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.7266 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3477 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4180 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4277 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6016 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2734 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2871 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0596 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.7344 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6953 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4668 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3008 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.2188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6602 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2539 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2676 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0781 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.6953 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3184 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4102 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2871 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6406 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=13.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4531 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=13.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4551 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=13.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0854 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.0312 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3262 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2930 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7344 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2490 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3691 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.9766 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3574 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4805 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3047 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3789 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4375 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0645 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.8047 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5156 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3457 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2461 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.3438 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2021 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3125 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0723 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2773 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2314 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.4688 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5938 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2520 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2305 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.1094 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4590 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4258 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.9141 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1777 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0996 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.1406 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7773 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4844 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3652 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.1562 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.9102 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3359 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3730 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0879 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.3594 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2207 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4883 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4570 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.7812 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8203 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4297 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4004 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1416 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.3281 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4668 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3809 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2812 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6328 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4023 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3633 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1367 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.9453 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4824 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4863 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2676 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8086 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3613 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0776 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.6719 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4688 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3984 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3164 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6172 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2139 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2832 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0869 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.6875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5156 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.6875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2363 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.6875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7891 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2656 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3027 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1025 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.6406 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5703 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4688 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2793 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.4375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8203 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2402 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3164 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0859 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.9766 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6094 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4004 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2852 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=13.4375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4570 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2246 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3379 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1240 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.9688 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2949 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5469 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5430 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=19.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2285 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3418 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1074 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.7188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3301 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3965 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1895 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5898 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2324 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4707 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1230 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4512 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1953 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=21.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6367 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2412 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1191 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.7031 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2988 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5234 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3535 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=23.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2314 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4336 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1348 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.6719 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3887 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3672 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2344 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=23.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2324 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3535 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1992 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.9844 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3398 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4238 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3320 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=34.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6992 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2246 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4082 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2578 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.9062 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4844 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4121 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2148 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=35.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5312 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (28): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2793 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3008 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1699 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.0312 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6211 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4902 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=36.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6680 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (29): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3086 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4004 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2539 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.0312 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3535 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4727 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=44.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7344 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (30): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2637 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3594 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2832 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5859 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=38.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4336 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (31): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4199 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5703 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3125 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.9688 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4375 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7695 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6719 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=141.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5469 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): QuantLinear(\n",
       "    in_features=4096, out_features=128256, bias=False\n",
       "    (input_quantizer): TensorQuantizer(disabled)\n",
       "    (output_quantizer): TensorQuantizer(disabled)\n",
       "    (weight_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtq.quantize(model, quantization_config, forward_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74e934",
   "metadata": {
    "papermill": {
     "duration": 0.004503,
     "end_time": "2026-01-21T05:24:06.817267",
     "exception": false,
     "start_time": "2026-01-21T05:24:06.812764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Start Quantization-Aware Training\n",
    "\n",
    "Configure CUDA environment variables for proper compilation and start the training process. The model will learn while simulating the effects of quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fdcb43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T05:24:06.827759Z",
     "iopub.status.busy": "2026-01-21T05:24:06.827637Z",
     "iopub.status.idle": "2026-01-21T08:47:58.552259Z",
     "shell.execute_reply": "2026-01-21T08:47:58.550113Z"
    },
    "papermill": {
     "duration": 12231.734105,
     "end_time": "2026-01-21T08:47:58.555821",
     "exception": false,
     "start_time": "2026-01-21T05:24:06.821716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2250/2250 3:23:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.198417</td>\n",
       "      <td>1.411486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.199400</td>\n",
       "      <td>1.086017</td>\n",
       "      <td>1.107087</td>\n",
       "      <td>51429.000000</td>\n",
       "      <td>0.705213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.981800</td>\n",
       "      <td>1.036240</td>\n",
       "      <td>1.088477</td>\n",
       "      <td>110540.000000</td>\n",
       "      <td>0.713918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.161500</td>\n",
       "      <td>1.090889</td>\n",
       "      <td>1.091327</td>\n",
       "      <td>165574.000000</td>\n",
       "      <td>0.703045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.277400</td>\n",
       "      <td>1.254704</td>\n",
       "      <td>1.229798</td>\n",
       "      <td>220373.000000</td>\n",
       "      <td>0.680800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.361100</td>\n",
       "      <td>1.348602</td>\n",
       "      <td>1.331394</td>\n",
       "      <td>272024.000000</td>\n",
       "      <td>0.666538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.551800</td>\n",
       "      <td>1.446006</td>\n",
       "      <td>1.183261</td>\n",
       "      <td>328039.000000</td>\n",
       "      <td>0.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>1.373749</td>\n",
       "      <td>1.354610</td>\n",
       "      <td>386991.000000</td>\n",
       "      <td>0.661280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.429800</td>\n",
       "      <td>1.365460</td>\n",
       "      <td>1.362019</td>\n",
       "      <td>441345.000000</td>\n",
       "      <td>0.663492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.583100</td>\n",
       "      <td>1.326017</td>\n",
       "      <td>1.427243</td>\n",
       "      <td>499140.000000</td>\n",
       "      <td>0.665701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.861100</td>\n",
       "      <td>1.435106</td>\n",
       "      <td>0.973985</td>\n",
       "      <td>544539.000000</td>\n",
       "      <td>0.661277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.792900</td>\n",
       "      <td>1.429581</td>\n",
       "      <td>0.932927</td>\n",
       "      <td>599860.000000</td>\n",
       "      <td>0.665052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>1.371125</td>\n",
       "      <td>1.060758</td>\n",
       "      <td>656645.000000</td>\n",
       "      <td>0.672187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.875300</td>\n",
       "      <td>1.355855</td>\n",
       "      <td>0.951992</td>\n",
       "      <td>717102.000000</td>\n",
       "      <td>0.675059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.823600</td>\n",
       "      <td>1.385058</td>\n",
       "      <td>0.947874</td>\n",
       "      <td>767833.000000</td>\n",
       "      <td>0.672301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.714800</td>\n",
       "      <td>1.356754</td>\n",
       "      <td>0.891943</td>\n",
       "      <td>826279.000000</td>\n",
       "      <td>0.679202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.740700</td>\n",
       "      <td>1.337558</td>\n",
       "      <td>0.890551</td>\n",
       "      <td>882453.000000</td>\n",
       "      <td>0.679205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.778800</td>\n",
       "      <td>1.296166</td>\n",
       "      <td>1.067894</td>\n",
       "      <td>936070.000000</td>\n",
       "      <td>0.686451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.763400</td>\n",
       "      <td>1.285874</td>\n",
       "      <td>0.926521</td>\n",
       "      <td>998280.000000</td>\n",
       "      <td>0.691528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.089900</td>\n",
       "      <td>1.436670</td>\n",
       "      <td>0.706780</td>\n",
       "      <td>1059309.000000</td>\n",
       "      <td>0.683693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.301800</td>\n",
       "      <td>1.387238</td>\n",
       "      <td>0.713625</td>\n",
       "      <td>1118717.000000</td>\n",
       "      <td>0.689789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.313400</td>\n",
       "      <td>1.379493</td>\n",
       "      <td>0.733444</td>\n",
       "      <td>1171561.000000</td>\n",
       "      <td>0.688958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>1.370131</td>\n",
       "      <td>0.711657</td>\n",
       "      <td>1222354.000000</td>\n",
       "      <td>0.689746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.289900</td>\n",
       "      <td>1.398499</td>\n",
       "      <td>0.663457</td>\n",
       "      <td>1276161.000000</td>\n",
       "      <td>0.693430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>1.383133</td>\n",
       "      <td>0.678749</td>\n",
       "      <td>1328009.000000</td>\n",
       "      <td>0.693667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>1.394715</td>\n",
       "      <td>0.620724</td>\n",
       "      <td>1380127.000000</td>\n",
       "      <td>0.696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>1.383069</td>\n",
       "      <td>0.639193</td>\n",
       "      <td>1439247.000000</td>\n",
       "      <td>0.698838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>1.349497</td>\n",
       "      <td>0.655235</td>\n",
       "      <td>1497420.000000</td>\n",
       "      <td>0.699995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>1.424786</td>\n",
       "      <td>0.595280</td>\n",
       "      <td>1549788.000000</td>\n",
       "      <td>0.698612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>1.459195</td>\n",
       "      <td>0.551074</td>\n",
       "      <td>1604930.000000</td>\n",
       "      <td>0.698241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>1.455293</td>\n",
       "      <td>0.549830</td>\n",
       "      <td>1658881.000000</td>\n",
       "      <td>0.699174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>1.494265</td>\n",
       "      <td>0.504670</td>\n",
       "      <td>1713826.000000</td>\n",
       "      <td>0.702235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>1.446506</td>\n",
       "      <td>0.542338</td>\n",
       "      <td>1768088.000000</td>\n",
       "      <td>0.703149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.111700</td>\n",
       "      <td>1.459779</td>\n",
       "      <td>0.521583</td>\n",
       "      <td>1830422.000000</td>\n",
       "      <td>0.701889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>1.455457</td>\n",
       "      <td>0.505543</td>\n",
       "      <td>1886982.000000</td>\n",
       "      <td>0.704677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>1.477261</td>\n",
       "      <td>0.499079</td>\n",
       "      <td>1939492.000000</td>\n",
       "      <td>0.705041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>1.458946</td>\n",
       "      <td>0.511980</td>\n",
       "      <td>1996560.000000</td>\n",
       "      <td>0.704528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>1.570202</td>\n",
       "      <td>0.456350</td>\n",
       "      <td>2053855.000000</td>\n",
       "      <td>0.703049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>1.578147</td>\n",
       "      <td>0.452167</td>\n",
       "      <td>2104640.000000</td>\n",
       "      <td>0.702428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>1.586049</td>\n",
       "      <td>0.447713</td>\n",
       "      <td>2156729.000000</td>\n",
       "      <td>0.703637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>1.586752</td>\n",
       "      <td>0.447076</td>\n",
       "      <td>2214314.000000</td>\n",
       "      <td>0.702953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>1.593188</td>\n",
       "      <td>0.444210</td>\n",
       "      <td>2270922.000000</td>\n",
       "      <td>0.702701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>1.589826</td>\n",
       "      <td>0.445211</td>\n",
       "      <td>2328311.000000</td>\n",
       "      <td>0.702599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>1.588602</td>\n",
       "      <td>0.445660</td>\n",
       "      <td>2386390.000000</td>\n",
       "      <td>0.703577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>1.585687</td>\n",
       "      <td>0.445857</td>\n",
       "      <td>2442102.000000</td>\n",
       "      <td>0.703491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>1.585871</td>\n",
       "      <td>0.446426</td>\n",
       "      <td>2495700.000000</td>\n",
       "      <td>0.703685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to output/checkpoint-450/modelopt_state.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to output/checkpoint-900/modelopt_state.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to output/checkpoint-1350/modelopt_state.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to output/checkpoint-1800/modelopt_state.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ModelOpt state to output/checkpoint-2250/modelopt_state.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2250, training_loss=0.5407062782181634, metrics={'train_runtime': 12231.5074, 'train_samples_per_second': 0.368, 'train_steps_per_second': 0.184, 'total_flos': 1.123802430234624e+17, 'train_loss': 0.5407062782181634, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"12.1\"\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "os.environ[\"PATH\"] = \"/usr/local/cuda/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6924e66b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T08:47:58.567738Z",
     "iopub.status.busy": "2026-01-21T08:47:58.567614Z",
     "iopub.status.idle": "2026-01-21T08:48:08.802074Z",
     "shell.execute_reply": "2026-01-21T08:48:08.800818Z"
    },
    "papermill": {
     "duration": 10.24186,
     "end_time": "2026-01-21T08:48:08.802834",
     "exception": false,
     "start_time": "2026-01-21T08:47:58.560974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/modelopt/torch/export/unified_export_hf.py:413: UserWarning: Model's original dtype (bfloat16) differs from target dtype (torch.bfloat16), which may lead to numerical errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./output/nvfp4/tokenizer_config.json',\n",
       " './output/nvfp4/special_tokens_map.json',\n",
       " './output/nvfp4/chat_template.jinja',\n",
       " './output/nvfp4/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from modelopt.torch.export import export_hf_checkpoint\n",
    "\n",
    "export_path = \"./output/nvfp4\"\n",
    "export_hf_checkpoint(model, dtype=torch.bfloat16, export_dir=export_path)\n",
    "tokenizer.save_pretrained(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48d1c1bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T08:48:08.811063Z",
     "iopub.status.busy": "2026-01-21T08:48:08.810986Z",
     "iopub.status.idle": "2026-01-21T08:48:08.814390Z",
     "shell.execute_reply": "2026-01-21T08:48:08.814121Z"
    },
    "papermill": {
     "duration": 0.008902,
     "end_time": "2026-01-21T08:48:08.815124",
     "exception": false,
     "start_time": "2026-01-21T08:48:08.806222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved config\n",
      " Created model card\n",
      "\n",
      "Model saved to ./output/nvfp4/\n",
      "Contents: ['special_tokens_map.json', 'README.md', 'model-00002-of-00002.safetensors', 'tokenizer_config.json', 'chat_template.jinja', 'hf_quant_config.json', 'generation_config.json', 'config.json', 'model-00001-of-00002.safetensors', 'model.safetensors.index.json', 'tokenizer.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save model config\n",
    "model.config.save_pretrained(export_path)\n",
    "print(\" Saved config\")\n",
    "\n",
    "# Create a README model card\n",
    "model_card = \"\"\"---\n",
    "license: llama3.1\n",
    "base_model: meta-llama/Llama-3.1-8B-Instruct\n",
    "tags:\n",
    "  - llama\n",
    "  - quantized\n",
    "  - nvidia-modeloptimizer\n",
    "  - NVFP4\n",
    "  - QAT \n",
    "library_name: nvidia-modeloptimizer\n",
    "---\n",
    "\n",
    "# Llama-3.1-8B-Instruct Quantized (ModelOpt NVFP4) through QAT\n",
    "\n",
    "This is a quantized version of [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) using [modelopt](https://github.com/NVIDIA/Model-Optimizer) with NVFP4 weight quantization.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model:** meta-llama/Llama-3.1-8B-Instruct\n",
    "- **Quantization Method:** modelopt NVFP4 Quantization Aware Training (QAT) \n",
    "- **Weight Precision:** NVFP4\n",
    "- **Original Size:** ~16 GB (bfloat16)\n",
    "- **Quantized Size:** ~6 GB (nvfp4)\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load base model structure\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Load tokenizer and generate\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4-QAT\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "This model inherits the [Llama 3.1 Community License](https://llama.meta.com/llama3_1/license/).\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{export_path}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "print(\" Created model card\")\n",
    "\n",
    "print(f\"\\nModel saved to {export_path}/\")\n",
    "print(\"Contents:\", os.listdir(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6810dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T08:48:08.822156Z",
     "iopub.status.busy": "2026-01-21T08:48:08.822053Z",
     "iopub.status.idle": "2026-01-21T08:49:55.046091Z",
     "shell.execute_reply": "2026-01-21T08:49:55.045484Z"
    },
    "papermill": {
     "duration": 106.229087,
     "end_time": "2026-01-21T08:49:55.046948",
     "exception": false,
     "start_time": "2026-01-21T08:48:08.817861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Repository created: tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4-QAT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be396a10b4640d492147c6f91451cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d92c64f34f417e81a7317b1e0778e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Uploaded to https://huggingface.co/tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4-QAT\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "# Load write token from .env file\n",
    "hf_write_token = os.getenv(\"HF_WRITE_TOKEN\")\n",
    "\n",
    "# Upload to HuggingFace Hub\n",
    "repo_name = \"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4-QAT\"  # Change to your username/repo\n",
    "\n",
    "try:\n",
    "    # Create the repo (set private=True if you want it private)\n",
    "    create_repo(repo_name, exist_ok=True, private=False, token=hf_write_token)\n",
    "    print(f\" Repository created: {repo_name}\")\n",
    "    \n",
    "    # Upload all files\n",
    "    upload_folder(\n",
    "        folder_path=export_path,\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload Llama-3.1-8B quantized with ModelOpt NVFP4-QAT\",\n",
    "        token=hf_write_token\n",
    "    )\n",
    "    print(f\" Uploaded to https://huggingface.co/{repo_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7c6c818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T08:49:55.054336Z",
     "iopub.status.busy": "2026-01-21T08:49:55.054181Z",
     "iopub.status.idle": "2026-01-21T08:49:55.067845Z",
     "shell.execute_reply": "2026-01-21T08:49:55.067118Z"
    },
    "papermill": {
     "duration": 0.0179,
     "end_time": "2026-01-21T08:49:55.068311",
     "exception": false,
     "start_time": "2026-01-21T08:49:55.050411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 6.0277502720000005 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")  # Add parent directory to path\n",
    "\n",
    "# Force reload the module to pick up changes\n",
    "import importlib\n",
    "import quantization_theory_helper\n",
    "importlib.reload(quantization_theory_helper)\n",
    "\n",
    "from quantization_theory_helper import compute_module_sizes\n",
    "module_size = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_size[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc02887",
   "metadata": {
    "papermill": {
     "duration": 0.002764,
     "end_time": "2026-01-21T08:49:55.073968",
     "exception": false,
     "start_time": "2026-01-21T08:49:55.071204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12504.347215,
   "end_time": "2026-01-21T08:49:58.387802",
   "environment_variables": {},
   "exception": null,
   "input_path": "NVFP4_QAT.ipynb",
   "output_path": "output_executed.ipynb",
   "parameters": {},
   "start_time": "2026-01-21T05:21:34.040587",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0837efabd4bc41aba93160502af951e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "14d80d98b4db4c68a037eb32a7be6a83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e0c52aa823a64ad8b3e420bb50852055",
        "IPY_MODEL_7b4f0f140cba45a5b05fe372827f230f",
        "IPY_MODEL_a6529c9e1f4a416aa1d149934b1c1325"
       ],
       "layout": "IPY_MODEL_e5721aced20242668f5d475c1ba8d8a9",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2054814a1a3147c0bd981ce4144e18e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "313a8b019e3d444784e9a740e8c243e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_773ade61cfd8476eb28f5773fef4ddd7",
       "placeholder": "",
       "style": "IPY_MODEL_9a158a1610b243be89731b5d8aa86365",
       "tabbable": null,
       "tooltip": null,
       "value": "5.77GB/5.77GB,23.3MB/s"
      }
     },
     "32a0fd09ee814de196f95f33eea264ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "36a57e91ccf94dc390d5ce0772989b2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "51b48b6a2e8348dd9b75d5dddc10fd3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ea6dd4a73ffc4b5eb08d8e6bc73e425a",
       "placeholder": "",
       "style": "IPY_MODEL_fbd2ace1bf4d48adbbc77f240ad7b757",
       "tabbable": null,
       "tooltip": null,
       "value": "ProcessingFiles(3/3):100%"
      }
     },
     "53d92c64f34f417e81a7317b1e0778e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d8cc7993d05a4e4c95cdd376627bc0b1",
        "IPY_MODEL_db6ad0ad21234dd481aea52235c9b1ef",
        "IPY_MODEL_313a8b019e3d444784e9a740e8c243e1"
       ],
       "layout": "IPY_MODEL_78a881c1c28f45ca8533e5fbd2858bd3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "584d38e8ed4e4ff8a9efcffe5748db53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d1e7e80da4c4499581914602803d2a6d",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c4eefdd17b024b6e97dcd7b5bf05cff3",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "5be396a10b4640d492147c6f91451cf4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_51b48b6a2e8348dd9b75d5dddc10fd3f",
        "IPY_MODEL_584d38e8ed4e4ff8a9efcffe5748db53",
        "IPY_MODEL_751aad276b7543ca8ba3f437ca051c20"
       ],
       "layout": "IPY_MODEL_82dc93fd31684dfe83af9fafe8045519",
       "tabbable": null,
       "tooltip": null
      }
     },
     "751aad276b7543ca8ba3f437ca051c20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b1746d35f3c7433388110552a6bee03b",
       "placeholder": "",
       "style": "IPY_MODEL_2054814a1a3147c0bd981ce4144e18e1",
       "tabbable": null,
       "tooltip": null,
       "value": "6.05GB/6.05GB,23.3MB/s"
      }
     },
     "773ade61cfd8476eb28f5773fef4ddd7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78a881c1c28f45ca8533e5fbd2858bd3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b4f0f140cba45a5b05fe372827f230f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fc2097b3fdae4972af3851b91744572b",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0d07507312448ca8502369bb9c5a751",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "82dc93fd31684dfe83af9fafe8045519": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9a158a1610b243be89731b5d8aa86365": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9b325637b9f749728bd5f2217b1f657c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "9e390b5f16b24684b7ba66609db7814e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a6529c9e1f4a416aa1d149934b1c1325": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9e390b5f16b24684b7ba66609db7814e",
       "placeholder": "",
       "style": "IPY_MODEL_cb757a1fc09e4235bb56e7bb93ba5880",
       "tabbable": null,
       "tooltip": null,
       "value": "4/4[01:21&lt;00:00,17.63s/it]"
      }
     },
     "a967799f230e405b96d90c35bc6c87df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b1746d35f3c7433388110552a6bee03b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c4eefdd17b024b6e97dcd7b5bf05cff3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cb757a1fc09e4235bb56e7bb93ba5880": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d0d07507312448ca8502369bb9c5a751": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d1e7e80da4c4499581914602803d2a6d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "d8cc7993d05a4e4c95cdd376627bc0b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f3549b8e68dc4b81a369514ef91ce2a1",
       "placeholder": "",
       "style": "IPY_MODEL_a967799f230e405b96d90c35bc6c87df",
       "tabbable": null,
       "tooltip": null,
       "value": "NewDataUpload:100%"
      }
     },
     "db6ad0ad21234dd481aea52235c9b1ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9b325637b9f749728bd5f2217b1f657c",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_32a0fd09ee814de196f95f33eea264ee",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "e0c52aa823a64ad8b3e420bb50852055": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0837efabd4bc41aba93160502af951e5",
       "placeholder": "",
       "style": "IPY_MODEL_36a57e91ccf94dc390d5ce0772989b2e",
       "tabbable": null,
       "tooltip": null,
       "value": "Loadingcheckpointshards:100%"
      }
     },
     "e5721aced20242668f5d475c1ba8d8a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea6dd4a73ffc4b5eb08d8e6bc73e425a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f3549b8e68dc4b81a369514ef91ce2a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fbd2ace1bf4d48adbbc77f240ad7b757": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fc2097b3fdae4972af3851b91744572b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}