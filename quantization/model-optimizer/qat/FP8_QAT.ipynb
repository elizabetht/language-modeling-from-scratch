{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e270624",
   "metadata": {},
   "source": [
    "## Install Required Dependencies\n",
    "\n",
    "Install the necessary packages for quantization-aware training including ipywidgets for notebook UI, nvidia-modelopt for quantization tools, and trl for transformer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657febd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ipywidgets in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (8.1.8)\n",
      "Requirement already satisfied: trl in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (0.22.2)\n",
      "Requirement already satisfied: nvidia-modelopt[all] in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (0.40.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (9.9.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: ninja in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.13.0)\n",
      "Requirement already satisfied: numpy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2.4.1)\n",
      "Requirement already satisfied: packaging in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2.12.5)\n",
      "Requirement already satisfied: nvidia-ml-py>=12 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (13.590.44)\n",
      "Requirement already satisfied: rich in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (14.2.0)\n",
      "Requirement already satisfied: scipy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (4.67.1)\n",
      "Requirement already satisfied: pulp in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (3.3.0)\n",
      "Requirement already satisfied: regex in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2026.1.15)\n",
      "Requirement already satisfied: safetensors in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.7.0)\n",
      "Requirement already satisfied: torch>=2.6 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (2.9.0+cu130)\n",
      "Requirement already satisfied: torchprofile>=0.0.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.0.4)\n",
      "Requirement already satisfied: cppimport in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (22.8.2)\n",
      "Requirement already satisfied: ml_dtypes in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.5.4)\n",
      "Requirement already satisfied: onnx-graphsurgeon in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.5.8)\n",
      "Requirement already satisfied: onnx~=1.19.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.19.1)\n",
      "Requirement already satisfied: onnxconverter-common~=1.16.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.16.0)\n",
      "Requirement already satisfied: onnxruntime~=1.22.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.22.1)\n",
      "Requirement already satisfied: onnxscript in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.5.7)\n",
      "Requirement already satisfied: onnxslim>=0.1.76 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.1.82)\n",
      "Requirement already satisfied: polygraphy>=0.49.22 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.49.26)\n",
      "Requirement already satisfied: accelerate>=1.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (1.12.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (4.3.0)\n",
      "Requirement already satisfied: diffusers>=0.32.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.36.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.24.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.36.0)\n",
      "Requirement already satisfied: peft>=0.17.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.18.1)\n",
      "Requirement already satisfied: transformers<5.0,>=4.53 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (4.55.4)\n",
      "Requirement already satisfied: deepspeed>=0.9.6 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt[all]) (0.18.4)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnx~=1.19.0->nvidia-modelopt[all]) (6.33.4)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnx~=1.19.0->nvidia-modelopt[all]) (4.15.0)\n",
      "Requirement already satisfied: coloredlogs in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxruntime~=1.22.0->nvidia-modelopt[all]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxruntime~=1.22.0->nvidia-modelopt[all]) (25.12.19)\n",
      "Requirement already satisfied: sympy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxruntime~=1.22.0->nvidia-modelopt[all]) (1.14.0)\n",
      "Requirement already satisfied: filelock in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (3.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (6.0.3)\n",
      "Requirement already satisfied: requests in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from transformers<5.0,>=4.53->nvidia-modelopt[all]) (0.21.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.24.0->nvidia-modelopt[all]) (1.2.0)\n",
      "Requirement already satisfied: psutil in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from accelerate>=1.0.0->nvidia-modelopt[all]) (7.2.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (0.4.0)\n",
      "Requirement already satisfied: pandas in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->nvidia-modelopt[all]) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (3.13.3)\n",
      "Requirement already satisfied: anyio in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (4.12.1)\n",
      "Requirement already satisfied: certifi in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.0.9)\n",
      "Requirement already satisfied: idna in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->nvidia-modelopt[all]) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0.0->nvidia-modelopt[all]) (1.22.0)\n",
      "Requirement already satisfied: einops in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (0.8.1)\n",
      "Requirement already satisfied: hjson in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (1.1.2)\n",
      "Requirement already satisfied: py-cpuinfo in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from deepspeed>=0.9.6->nvidia-modelopt[all]) (9.0.0)\n",
      "Requirement already satisfied: importlib_metadata in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from diffusers>=0.32.2->nvidia-modelopt[all]) (8.7.1)\n",
      "Requirement already satisfied: Pillow in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from diffusers>=0.32.2->nvidia-modelopt[all]) (12.1.0)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: colorama in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxslim>=0.1.76->nvidia-modelopt[all]) (0.4.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt[all]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt[all]) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt[all]) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from requests->transformers<5.0,>=4.53->nvidia-modelopt[all]) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from requests->transformers<5.0,>=4.53->nvidia-modelopt[all]) (2.6.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from sympy->onnxruntime~=1.22.0->nvidia-modelopt[all]) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (80.9.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (9.13.0.50)\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.0.19)\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (12.0.0.15)\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (10.4.0.35)\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (12.0.3.29)\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (12.6.2.49)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (0.8.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (2.27.7)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.3.24)\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.39)\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (13.0.39)\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (1.15.0.42)\n",
      "Requirement already satisfied: triton==3.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt[all]) (3.5.0)\n",
      "Requirement already satisfied: torchvision>=0.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torchprofile>=0.0.4->nvidia-modelopt[all]) (0.24.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime~=1.22.0->nvidia-modelopt[all]) (10.0)\n",
      "Requirement already satisfied: mako in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from cppimport->nvidia-modelopt[all]) (1.3.10)\n",
      "Requirement already satisfied: pybind11 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from cppimport->nvidia-modelopt[all]) (3.0.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from importlib_metadata->diffusers>=0.32.2->nvidia-modelopt[all]) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.6->nvidia-modelopt[all]) (3.0.3)\n",
      "Requirement already satisfied: onnx_ir<2,>=0.1.12 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from onnxscript->nvidia-modelopt[all]) (0.1.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->nvidia-modelopt[all]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->nvidia-modelopt[all]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->nvidia-modelopt[all]) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->nvidia-modelopt[all]) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from rich->nvidia-modelopt[all]) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->nvidia-modelopt[all]) (0.1.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets nvidia-modelopt[all] trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5ef1f",
   "metadata": {},
   "source": [
    "## Load Environment Variables and Configure Model\n",
    "\n",
    "Load Hugging Face authentication token from environment and specify the base model to use for quantization-aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2edf1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_WRITE_TOKEN\")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81efff24",
   "metadata": {},
   "source": [
    "## Enable Hugging Face Checkpointing\n",
    "\n",
    "Enable ModelOpt's Hugging Face checkpointing feature to ensure quantized models can be properly saved and loaded with Hugging Face's transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d53c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu130 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelOpt save/restore enabled for `transformers` library.\n",
      "ModelOpt save/restore enabled for `diffusers` library.\n",
      "ModelOpt save/restore enabled for `peft` library.\n"
     ]
    }
   ],
   "source": [
    "import modelopt.torch.opt as mto\n",
    "mto.enable_huggingface_checkpointing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a15e72",
   "metadata": {},
   "source": [
    "## Configure Model Arguments\n",
    "\n",
    "Define model configuration including attention implementation, precision (bfloat16), and device settings. Setting `use_cache=False` is required for training to enable gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95899a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import ModelConfig\n",
    "\n",
    "model_args = ModelConfig(\n",
    "    model_name_or_path=model_name,\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model_kwargs = {\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "    \"attn_implementation\": model_args.attn_implementation,\n",
    "    \"torch_dtype\": model_args.torch_dtype,\n",
    "    \"use_cache\": False,\n",
    "    \"device_map\": \"auto\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3b189",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "Load the pre-trained Llama model and its corresponding tokenizer from Hugging Face Hub using the configurations defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0891b3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56c420d0ac04008b358f33495e8db77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953a4a4",
   "metadata": {},
   "source": [
    "## Configure Dataset Arguments\n",
    "\n",
    "Specify the dataset to use for training (Multilingual-Thinking dataset) and define the train/test split configuration with 10% held out for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1097745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import ScriptArguments\n",
    "\n",
    "script_args = ScriptArguments(\n",
    "    dataset_name=\"HuggingFaceH4/Multilingual-Thinking\",\n",
    "    dataset_train_split=\"train\",\n",
    "    dataset_test_split=\"test\",\n",
    ")\n",
    "test_size = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bc0f6",
   "metadata": {},
   "source": [
    "## Load and Split Dataset\n",
    "\n",
    "Load the multilingual reasoning dataset and split it into training and evaluation sets using a fixed seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e407c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(script_args.dataset_name)\n",
    "# split the dataset into train and test\n",
    "dataset = dataset[script_args.dataset_train_split].train_test_split(test_size=test_size, seed=42)\n",
    "train_dataset = dataset[script_args.dataset_train_split]\n",
    "eval_dataset = dataset[script_args.dataset_test_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028492e",
   "metadata": {},
   "source": [
    "## Configure Training Arguments\n",
    "\n",
    "Set up training hyperparameters including learning rate, batch size, evaluation strategy, and checkpointing configuration. The training will run for 1 epoch with evaluation every 50 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e407c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_length=4096,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_on_start=True,\n",
    "    logging_steps=25,\n",
    "    save_steps=450,\n",
    "    eval_steps=50,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5354e1",
   "metadata": {},
   "source": [
    "## Initialize SFT Trainer\n",
    "\n",
    "Create a Supervised Fine-Tuning (SFT) trainer that will handle the training loop, evaluation, and model optimization during quantization-aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93fb06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[script_args.dataset_train_split],\n",
    "    eval_dataset=dataset[script_args.dataset_test_split],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc3bb9",
   "metadata": {},
   "source": [
    "## Setup Quantization Configuration\n",
    "\n",
    "Configure quantization settings (NVFP4 format) and prepare calibration data. The forward loop function is defined to run calibration samples through the model for quantization parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import modelopt.torch.quantization as mtq\n",
    "\n",
    "# Some configs don't need calibration, but other quantization configurations may require it.\n",
    "quantization_config = mtq.FP8_DEFAULT_CFG\n",
    "calib_size = 512\n",
    "\n",
    "dataset = torch.utils.data.Subset(\n",
    "    trainer.eval_dataset, list(range(min(len(trainer.eval_dataset), calib_size)))\n",
    ")\n",
    "data_loader = trainer.get_eval_dataloader(dataset)\n",
    "\n",
    "\n",
    "def forward_loop(model):\n",
    "    for data in data_loader:\n",
    "        model(**data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a5770",
   "metadata": {},
   "source": [
    "## Apply Quantization to Model\n",
    "\n",
    "Apply the quantization configuration to the model using the calibration data. This prepares the model for quantization-aware training by inserting quantization layers and running calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b6925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered <class 'transformers.models.llama.modeling_llama.LlamaAttention'> to _QuantAttention for KV Cache quantization\n",
      "Inserted 771 quantizers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7461 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6992 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0610 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4180 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4629 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1963 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6133 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5352 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3555 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0967 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8711 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4141 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.3438 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5273 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.3438 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4629 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=478.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2617 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3496 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0586 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.3594 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3457 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.7188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3242 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.7188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2480 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.9219 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6055 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2188 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2832 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1357 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.2578 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3770 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.7812 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5078 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.7812 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2754 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.0938 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5469 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3301 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3359 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0742 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.1406 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3574 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3047 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.5938 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6562 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2578 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5273 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1357 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.7266 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3477 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4180 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4277 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6016 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2734 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2871 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0596 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.7344 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6953 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4668 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3008 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.2188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6602 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2539 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2676 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=11.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0781 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.6953 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3184 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4102 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2871 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6406 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=13.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4531 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=13.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4551 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=13.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0854 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.0312 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3262 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2930 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7344 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2490 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3691 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.9766 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3574 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4805 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3047 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3789 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4375 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0645 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.8047 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5156 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3457 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2461 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.3438 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2021 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3125 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0723 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2773 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2314 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.4688 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5938 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2520 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2305 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.1094 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4590 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4258 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.9141 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1777 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0996 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.1406 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7773 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4844 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3652 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.1562 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.9102 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3359 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3730 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0879 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.3594 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2207 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4883 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4570 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.7812 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8203 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4297 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4004 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1416 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.3281 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4668 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3809 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.3125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2812 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6328 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4023 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3633 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1367 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.9453 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4824 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4863 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2676 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8086 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3613 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0776 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.6719 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4688 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3984 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3164 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6172 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2139 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2832 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0869 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.6875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5156 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=7.6875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2363 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.6875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7891 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2656 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3027 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1025 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.6406 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5703 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4688 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2793 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.4375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8203 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2402 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3164 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.0859 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=1.9766 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6094 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4004 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2852 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=13.4375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4570 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2246 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3379 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1240 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=2.9688 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2949 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5469 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5430 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=19.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.8281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (22): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2285 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3418 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=18.2500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1074 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.7188 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3301 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3965 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.8125 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1895 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5898 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (23): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2324 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4707 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1230 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4512 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1953 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=21.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6367 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2412 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3281 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=16.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1191 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.7031 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2988 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5234 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.9375 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3535 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=23.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5977 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2314 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4336 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.8750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1348 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.6719 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3887 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3672 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=8.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2344 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=23.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2324 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3535 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=17.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1992 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.9844 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3398 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4238 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3320 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=34.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6992 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2246 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4082 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=15.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2578 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=3.9062 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4844 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4121 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2148 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=35.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5312 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (28): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2793 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3008 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.1699 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.0312 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6211 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4902 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=10.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3926 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=36.7500 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6680 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (29): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3086 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4004 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2539 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=5.0312 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3535 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4727 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=12.6250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=44.5000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7344 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (30): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2637 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3594 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.3750 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2070 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=4.1250 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.2832 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5859 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.1875 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3867 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=38.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4336 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (31): LlamaDecoderLayer(\n",
       "        (self_attn): QuantLlamaAttention(\n",
       "          (q_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4199 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (k_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5703 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (v_proj): QuantLinear(\n",
       "            in_features=4096, out_features=1024, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=9.5625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.3125 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (o_proj): QuantLinear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=6.9688 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.4375 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (q_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (k_bmm_quantizer): TensorQuantizer(disabled)\n",
       "          (v_bmm_quantizer): TensorQuantizer(disabled)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.7695 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (up_proj): QuantLinear(\n",
       "            in_features=4096, out_features=14336, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=14.0625 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.6719 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (down_proj): QuantLinear(\n",
       "            in_features=14336, out_features=4096, bias=False\n",
       "            (input_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=141.0000 calibrator=MaxCalibrator quant)\n",
       "            (output_quantizer): TensorQuantizer(disabled)\n",
       "            (weight_quantizer): TensorQuantizer((2, 1) bit fake block_sizes={-1: 16, 'type': 'dynamic', 'scale_bits': (4, 3)}, amax=0.5469 calibrator=MaxCalibrator quant)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): QuantLinear(\n",
       "    in_features=4096, out_features=128256, bias=False\n",
       "    (input_quantizer): TensorQuantizer(disabled)\n",
       "    (output_quantizer): TensorQuantizer(disabled)\n",
       "    (weight_quantizer): TensorQuantizer(disabled)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtq.quantize(model, quantization_config, forward_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74e934",
   "metadata": {},
   "source": [
    "## Start Quantization-Aware Training\n",
    "\n",
    "Configure CUDA environment variables for proper compilation and start the training process. The model will learn while simulating the effects of quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fdcb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  94/2250 07:38 < 2:59:03, 0.20 it/s, Epoch 0.21/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.198417</td>\n",
       "      <td>1.411486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.199400</td>\n",
       "      <td>1.086017</td>\n",
       "      <td>1.107087</td>\n",
       "      <td>51429.000000</td>\n",
       "      <td>0.705213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"12.1\"\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "os.environ[\"PATH\"] = \"/usr/local/cuda/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modelopt.torch.export import export_hf_checkpoint\n",
    "\n",
    "export_path = \"./output/fp8\"\n",
    "export_hf_checkpoint(model, dtype=torch.bfloat16, export_dir=export_path)\n",
    "tokenizer.save_pretrained(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save model config\n",
    "model.config.save_pretrained(export_path)\n",
    "print(\" Saved config\")\n",
    "\n",
    "# Create a README model card\n",
    "model_card = \"\"\"---\n",
    "license: llama3.1\n",
    "base_model: meta-llama/Llama-3.1-8B-Instruct\n",
    "tags:\n",
    "  - llama\n",
    "  - quantized\n",
    "  - nvidia-modeloptimizer\n",
    "  - FP8\n",
    "  - QAT \n",
    "library_name: nvidia-modeloptimizer\n",
    "---\n",
    "\n",
    "# Llama-3.1-8B-Instruct Quantized (ModelOpt FP8) through QAT\n",
    "\n",
    "This is a quantized version of [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) using [modelopt](https://github.com/NVIDIA/Model-Optimizer) with NVFP4 weight quantization.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model:** meta-llama/Llama-3.1-8B-Instruct\n",
    "- **Quantization Method:** modelopt FP8 Quantization Aware Training (QAT) \n",
    "- **Weight Precision:** FP8\n",
    "- **Original Size:** ~16 GB (bfloat16)\n",
    "- **Quantized Size:** ~6 GB (fp8)\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load base model structure\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-FP8\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Load tokenizer and generate\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4-QAT\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "This model inherits the [Llama 3.1 Community License](https://llama.meta.com/llama3_1/license/).\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{export_path}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "print(\" Created model card\")\n",
    "\n",
    "print(f\"\\nModel saved to {export_path}/\")\n",
    "print(\"Contents:\", os.listdir(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6810dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "# Load write token from .env file\n",
    "hf_write_token = os.getenv(\"HF_WRITE_TOKEN\")\n",
    "\n",
    "# Upload to HuggingFace Hub\n",
    "repo_name = \"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-FP8-QAT\"  # Change to your username/repo\n",
    "\n",
    "try:\n",
    "    # Create the repo (set private=True if you want it private)\n",
    "    create_repo(repo_name, exist_ok=True, private=False, token=hf_write_token)\n",
    "    print(f\" Repository created: {repo_name}\")\n",
    "    \n",
    "    # Upload all files\n",
    "    upload_folder(\n",
    "        folder_path=export_path,\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload Llama-3.1-8B quantized with ModelOpt FP8-QAT\",\n",
    "        token=hf_write_token\n",
    "    )\n",
    "    print(f\" Uploaded to https://huggingface.co/{repo_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")  # Add parent directory to path\n",
    "\n",
    "# Force reload the module to pick up changes\n",
    "import importlib\n",
    "import quantization_theory_helper\n",
    "importlib.reload(quantization_theory_helper)\n",
    "\n",
    "from quantization_theory_helper import compute_module_sizes\n",
    "module_size = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_size[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc02887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
