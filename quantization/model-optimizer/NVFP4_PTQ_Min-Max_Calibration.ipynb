{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44c8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"12.1\"\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "os.environ[\"PATH\"] = \"/usr/local/cuda/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7699a343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.9.0+cu130\n",
      "Uninstalling torch-2.9.0+cu130:\n",
      "  Successfully uninstalled torch-2.9.0+cu130\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu130\n",
      "Collecting torch==2.9.0\n",
      "  Using cached https://download.pytorch.org/whl/cu130/torch-2.9.0%2Bcu130-cp312-cp312-manylinux_2_28_aarch64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: torchvision==0.24.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: torchaudio==2.9.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (9.13.0.50)\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (13.0.0.19)\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (12.0.0.15)\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (10.4.0.35)\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (12.0.3.29)\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (12.6.2.49)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (0.8.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (2.27.7)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (3.3.24)\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (13.0.39)\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (13.0.39)\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (1.15.0.42)\n",
      "Requirement already satisfied: triton==3.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch==2.9.0) (3.5.0)\n",
      "Requirement already satisfied: numpy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torchvision==0.24.0) (2.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torchvision==0.24.0) (12.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.9.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from jinja2->torch==2.9.0) (3.0.3)\n",
      "Using cached https://download.pytorch.org/whl/cu130/torch-2.9.0%2Bcu130-cp312-cp312-manylinux_2_28_aarch64.whl (512.4 MB)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.9.0+cu130\n",
      "zsh:1: 0.43.2 not found\n",
      "Requirement already satisfied: nvidia-modelopt in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (0.40.0)\n",
      "Requirement already satisfied: ninja in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (1.13.0)\n",
      "Requirement already satisfied: numpy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (2.4.0)\n",
      "Requirement already satisfied: packaging in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (2.12.5)\n",
      "Requirement already satisfied: nvidia-ml-py>=12 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (13.590.44)\n",
      "Requirement already satisfied: rich in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (14.2.0)\n",
      "Requirement already satisfied: scipy in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (1.16.3)\n",
      "Requirement already satisfied: tqdm in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (4.67.1)\n",
      "Requirement already satisfied: pulp in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (3.3.0)\n",
      "Requirement already satisfied: regex in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (2025.11.3)\n",
      "Requirement already satisfied: safetensors in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (0.7.0)\n",
      "Requirement already satisfied: torch>=2.6 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (2.9.0+cu130)\n",
      "Requirement already satisfied: torchprofile>=0.0.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from nvidia-modelopt) (0.0.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from pydantic>=2.0->nvidia-modelopt) (0.4.2)\n",
      "Requirement already satisfied: filelock in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (3.20.2)\n",
      "Requirement already satisfied: setuptools in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (9.13.0.50)\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (13.0.0.19)\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (12.0.0.15)\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (10.4.0.35)\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (12.0.3.29)\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (12.6.2.49)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (0.8.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (2.27.7)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (3.3.24)\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (13.0.39)\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (13.0.39)\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (1.15.0.42)\n",
      "Requirement already satisfied: triton==3.5.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torch>=2.6->nvidia-modelopt) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.6->nvidia-modelopt) (1.3.0)\n",
      "Requirement already satisfied: torchvision>=0.4 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torchprofile>=0.0.4->nvidia-modelopt) (0.24.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from torchvision>=0.4->torchprofile>=0.0.4->nvidia-modelopt) (12.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.6->nvidia-modelopt) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from rich->nvidia-modelopt) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from rich->nvidia-modelopt) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->nvidia-modelopt) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell first, then restart the kernel before running the next cell\n",
    "!pip uninstall torch -y\n",
    "!pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 --index-url https://download.pytorch.org/whl/cu130\n",
    "\n",
    "!pip install bitsandbytes>=0.43.2\n",
    "!pip install nvidia-modelopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21395453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu130\n",
      "CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run this cell after restarting the kernel\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "import modelopt.torch.quantization as mtq\n",
    "from modelopt.torch.utils.dataset_utils import create_forward_loop, get_dataset_dataloader\n",
    "\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c2a2a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813fbc270c414c0f97bb52cf5fe99e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "dataset_name = \"cnn_dailymail\"\n",
    "batch_size = 8\n",
    "calib_samples = 128\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdcd9e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0634b1633924983a35c1e0642191cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 16.060522752 GB\n"
     ]
    }
   ],
   "source": [
    "# Load model - use device_map=\"auto\" to handle device placement automatically\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    dtype=torch.bfloat16,  # Must be torch_dtype, not dtype\n",
    "    device_map=\"auto\",           # Don't use .cuda(), use this instead\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Add parent directory to path\n",
    "\n",
    "# Force reload the module to pick up changes\n",
    "import importlib\n",
    "import quantization_theory_helper\n",
    "importlib.reload(quantization_theory_helper)\n",
    "\n",
    "from quantization_theory_helper import compute_module_sizes\n",
    "module_size = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_size[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6da7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/src/github.com/elizabetht/language-modeling-from-scratch/.venv/lib/python3.12/site-packages/modelopt/torch/utils/dataset_utils.py:198: UserWarning: Tokenizer with the right padding_side may impact calibration accuracy. Recommend set to left\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader = get_dataset_dataloader(\n",
    "    dataset_name=dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    num_samples=calib_samples,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf3a3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_loop = create_forward_loop(dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46e7b41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered <class 'transformers.models.llama.modeling_llama.LlamaAttention'> to _QuantAttention for KV Cache quantization\n",
      "Inserted 771 quantizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:35<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Try FP8 instead of FP4 - may have better Blackwell support\n",
    "# If FP8 also fails, Triton doesn't support Blackwell GPUs yet for ModelOpt\n",
    "quant_config = mtq.NVFP4_DEFAULT_CFG  # Changed from NVFP4_DEFAULT_CFG\n",
    "model = mtq.quantize(model, quant_config, forward_loop=forward_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8cd037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Jessica and I'm a 25-year-old woman\n"
     ]
    }
   ],
   "source": [
    "# Using eager mode (no torch.compile) for Blackwell GPU compatibility\n",
    "inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e02a26e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./quantized_model/NVFP4/tokenizer_config.json',\n",
       " './quantized_model/NVFP4/special_tokens_map.json',\n",
       " './quantized_model/NVFP4/chat_template.jinja',\n",
       " './quantized_model/NVFP4/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelopt.torch.export import export_hf_checkpoint\n",
    "\n",
    "export_path = \"./quantized_model/NVFP4/\"\n",
    "export_hf_checkpoint(model, export_dir=export_path)\n",
    "tokenizer.save_pretrained(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf4514ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model size is 6.0277502720000005 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Add parent directory to path\n",
    "\n",
    "# Force reload the module to pick up changes\n",
    "import importlib\n",
    "import quantization_theory_helper\n",
    "importlib.reload(quantization_theory_helper)\n",
    "\n",
    "from quantization_theory_helper import compute_module_sizes\n",
    "module_size = compute_module_sizes(model)\n",
    "print(f\"The model size is {module_size[''] * 1e-9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e99eab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved config\n",
      "✓ Created model card\n",
      "\n",
      "Model saved to ./quantized_model/NVFP4//\n",
      "Contents: ['README.md', 'tokenizer_config.json', 'chat_template.jinja', 'special_tokens_map.json', 'generation_config.json', 'config.json', 'hf_quant_config.json', 'tokenizer.json', 'model-00002-of-00002.safetensors', 'model-00001-of-00002.safetensors', 'model.safetensors.index.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Save model config\n",
    "model.config.save_pretrained(export_path)\n",
    "print(\"✓ Saved config\")\n",
    "\n",
    "# Create a README model card\n",
    "model_card = \"\"\"---\n",
    "license: llama3.1\n",
    "base_model: meta-llama/Llama-3.1-8B-Instruct\n",
    "tags:\n",
    "  - llama\n",
    "  - quantized\n",
    "  - nvidia-modeloptimizer\n",
    "  - NVFP4\n",
    "library_name: nvidia-modeloptimizer\n",
    "---\n",
    "\n",
    "# Llama-3.1-8B-Instruct Quantized (ModelOpt NVFP4)\n",
    "\n",
    "This is a quantized version of [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) using [modelopt](https://github.com/NVIDIA/Model-Optimizer) with NVFP4 weight quantization.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model:** meta-llama/Llama-3.1-8B-Instruct\n",
    "- **Quantization Method:** modelopt NVFP4 Post-Training Quantization (PTQ)    \n",
    "- **Weight Precision:** NVFP4\n",
    "- **Original Size:** ~16 GB (bfloat16)\n",
    "- **Quantized Size:** ~6 GB (nvfp4)\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load base model structure\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Load tokenizer and generate\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "This model inherits the [Llama 3.1 Community License](https://llama.meta.com/llama3_1/license/).\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"{export_path}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "print(\"✓ Created model card\")\n",
    "\n",
    "print(f\"\\nModel saved to {export_path}/\")\n",
    "print(\"Contents:\", os.listdir(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6868343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Repository created: tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547bbf1aee5c4823bd6f859a8644cb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935abf5ff9c24dd4bd8fd7edbc4c5f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Uploaded to https://huggingface.co/tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "# Upload to HuggingFace Hub\n",
    "repo_name = \"tokenlabsdotrun/Llama-3.1-8B-ModelOpt-NVFP4\"  # Change to your username/repo\n",
    "\n",
    "try:\n",
    "    # Create the repo (set private=True if you want it private)\n",
    "    create_repo(repo_name, exist_ok=True, private=False)\n",
    "    print(f\"✓ Repository created: {repo_name}\")\n",
    "    \n",
    "    # Upload all files\n",
    "    upload_folder(\n",
    "        folder_path=export_path,\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload Llama-3.1-8B quantized with ModelOpt NVFP4\"\n",
    "    )\n",
    "    print(f\"✓ Uploaded to https://huggingface.co/{repo_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ff4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
